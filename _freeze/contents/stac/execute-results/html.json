{
  "hash": "116e4ab31a17163b715a405aa5d6f15f",
  "result": {
    "markdown": "---\ntitle: 'Examining Environmental Justice through Open Source, Cloud-Native Tools'\n\n---\n\n\nThis notebook provides a high-level overview illustrating a cloud-native workflow in both R and python.  \n\n::: {.panel-tabset}\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\nlibrary(dplyr)\ngdalcubes::gdalcubes_options(parallel = TRUE)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport odc.stac\nfrom pystac_client import Client\n```\n:::\n\n\n\n:::\n\n\n\n## Escaping the file paradigm\n\nLarge geospatial data comes in many different formats and is frequently divided into many individual files or \"assets\" which may represent different points in space, time, sensor bands or variables. Many users are familiar with file-based workflows, where each file type is read into the computational environment by a specific tool and that workflows proceed file-by-file. However, the same data can be represented in many different formats (NetCDF or tiff, say) and subdivided in different ways. Importantly, the file-based-divisions often do not reflect the way a user might want to work with the data.  For instance, a NASA product may provide sea-surface-temperature as one NetCDF file per day, with each file covering the entire global extent, while a user wants to examine trends in the data over time but only in a certain regional area. In such cases, it is inefficient to download data for the whole globe over many files.  Just as end-users in high level languages are not expected to manage very low-level concepts like memory block sizes, geospatial data scientists need not worry about these file serialization details when they have good high-level abstractions that can do it for them.  \n\n\n\n\n::: {.panel-tabset}\n\n## R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox <- c(xmin=-122.51, ymin=37.71, xmax=-122.36, ymax=37.81) \nstart_date <- \"2022-06-01\"\nend_date <- \"2022-08-01\"\nitems <-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |>\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |>\n  post_request()\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbox = [-122.51, 37.71, -122.36, 37.81]\nitems = (\n  Client.\n  open(\"https://earth-search.aws.element84.com/v1\").\n  search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}).\n  get_all_items()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/home/cboettig/.local/lib/python3.10/site-packages/pystac_client/item_search.py:850: FutureWarning: get_all_items() is deprecated, use item_collection() instead.\n  warnings.warn(\n```\n:::\n:::\n\n\n:::\n\nWe can build an abstract data cube from the assets and metadata returned by the STAC catalog:\n\n\n::: {.panel-tabset}\n\n## R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncol <- stac_image_collection(items$features,\n                             asset_names = c(\"B04\", \"B08\", \"SCL\"),\n                             property_filter = \\(x){\n                               x[[\"eo:cloud_cover\"]] < 20\n                             })\n\ncube <- cube_view(srs = \"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  nx = 1200, ny = 1200, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask <- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:32610\",\n    bands=[\"nir08\", \"red\"],\n    resolution=30,\n    bbox=box\n)\n```\n:::\n\n\n:::\n\nWe can calculate NDVI\n\n::: {.panel-tabset}\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nndvi <- raster_cube(col, cube, mask = mask) |>\n  select_bands(c(\"B04\", \"B08\")) |>\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |>\n  reduce_time(c(\"mean(NDVI)\"))\n\nndvi_stars <- st_as_stars(ndvi)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nred = data.red\nnir = data.nir08\n\nndvi = (((nir - red) / (red + nir)).\n        resample(time=\"MS\").\n        median(\"time\", keep_attrs=True).\n        compute()\n)\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi <= 1)\n```\n:::\n\n\n:::\n\n## Zonal statistics \n\nWe can also extract values at particular points or values falling within particular polygons.  One common operation is to summarise the values of all pixels falling within a given polygon, e.g. by the average value. \n\nHere, we examine the present-day impact of historic \"red-lining\" of US cities during the Great Depression using data from the [Mapping Inequality](https://dsl.richmond.edu/panorama/redlining) project.  All though this racist practice was banned by federal law under the Fair Housing Act of 1968, the systemic scars of that practice are still so deeply etched on our landscape that the remain visible from space -- \"red-lined\" areas (graded \"D\" under the racist HOLC scheme) show systematically lower greenness than predominately-white neighborhoods (Grade \"A\"):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsf <- st_read(\"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/CASanFrancisco1937.geojson\") |>\n  st_make_valid()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `CASanFrancisco1937' from data source \n  `/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/CASanFrancisco1937.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 97 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.5101 ymin: 37.70801 xmax: -122.3627 ymax: 37.80668\nGeodetic CRS:  WGS 84\n```\n:::\n\n```{.r .cell-code}\npoly <- ndvi |> extract_geom(sf, FUN = mean, reduce_time = TRUE)\nsf$NDVI <- poly$NDVI\n```\n:::\n\n\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it's textual grade, using `tmap`.  Note that \"A\" grades tend to be darkest green (high NDVI) while \"D\" grades are frequently the least green.  (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmako <- tm_scale_continuous(values = viridisLite::mako(30))\nfill <- tm_scale_continuous(values = \"Greens\")\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako) +\n  tm_shape(sf) + tm_polygons('NDVI', fill.scale = fill) +\n  tm_shape(sf) + tm_text(\"holc_grade\", col=\"darkblue\", size=0.6) +\n  tm_legend_hide()\n```\n\n::: {.cell-output-display}\n![](stac_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nAre historically redlined areas still less green?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsf |> \n  as_tibble() |>\n  group_by(holc_grade) |> \n  summarise(ndvi = mean(NDVI), \n            sd = sd(NDVI)) |>\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|holc_grade |      ndvi|        sd|\n|:----------|---------:|---------:|\n|A          | 0.3201041| 0.0612713|\n|B          | 0.2135646| 0.0784628|\n|C          | 0.1955263| 0.0564935|\n|D          | 0.1948817| 0.0385969|\n:::\n:::\n",
    "supporting": [
      "stac_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}