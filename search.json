[
  {
    "objectID": "tutorials/python/intro-python.html",
    "href": "tutorials/python/intro-python.html",
    "title": "Exploring the Legacy of Redlining",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing.\n\nfrom pystac_client import Client\nimport odc.stac\nimport pystac_client\nimport rioxarray\nimport geopandas as gpd\nfrom rasterstats import zonal_stats \n\n# suppress warning messages\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nThe first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox = [-122.51, 37.71, -122.36, 37.81]\nitems = (\n  Client.\n  open(\"https://earth-search.aws.element84.com/v1\").\n  search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}).\n  item_collection()\n)\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:32610\",\n    bands=[\"nir08\", \"red\"],\n    bbox=box\n)\n# For some reason, requesting reprojection in EPSG:4326 gives only empty values\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine vegetation cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi = (((data.nir08 - data.red) / (data.red + data.nir08)).\n        resample(time=\"MS\").\n        median(\"time\", keep_attrs=True).\n        compute()\n)\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=4)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fa18b685fc0&gt;\n\n\n\n\n\n\n\n\n\nWe examine the present-day impact of historic “red-lining” of US cities during the Great Depression using data from the Mapping Inequality project. All though this racist practice was banned by federal law under the Fair Housing Act of 1968, the systemic scars of that practice are still so deeply etched on our landscape that the remain visible from space – “red-lined” areas (graded “D” under the racist HOLC scheme) show systematically lower greenness than predominately-white neighborhoods (Grade “A”). Trees provide many benefits, from mitigating urban heat to biodiversity, real-estate value, to health.\n\n\nIn addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\n# make sure we are in the same projection.\n# zonal_stats method understands paths but not xarray\n(   ndvi.\n    rio.reproject(\"EPSG:4326\").\n    rio.to_raster(raster_path=\"ndvi.tif\", driver=\"COG\")   \n)\n\nsf_url = \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\"\nmean_ndvi = zonal_stats(sf_url, \"ndvi.tif\", stats=\"mean\")\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\nsf = gpd.read_file(sf_url)\nsf[\"ndvi\"] = [x[\"mean\"] for x in mean_ndvi]\nsf.plot(column=\"ndvi\", legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nAre historically redlined areas still less green?\n\nimport geopolars as gpl\nimport polars as pl\n\n(gpl.\n  from_geopandas(sf).\n  group_by(\"grade\").\n  agg(pl.col(\"ndvi\").mean()).\n  sort(\"grade\")\n)\n\n\nshape: (5, 2)\n\n\n\ngrade\nndvi\n\n\nstr\nf64\n\n\n\n\nnull\n0.159305\n\n\n\"A\"\n0.337976\n\n\n\"B\"\n0.247661\n\n\n\"C\"\n0.236407\n\n\n\"D\"\n0.232125"
  },
  {
    "objectID": "tutorials/python/intro-python.html#data-discovery",
    "href": "tutorials/python/intro-python.html#data-discovery",
    "title": "Exploring the Legacy of Redlining",
    "section": "",
    "text": "The first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox = [-122.51, 37.71, -122.36, 37.81]\nitems = (\n  Client.\n  open(\"https://earth-search.aws.element84.com/v1\").\n  search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}).\n  item_collection()\n)\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:32610\",\n    bands=[\"nir08\", \"red\"],\n    bbox=box\n)\n# For some reason, requesting reprojection in EPSG:4326 gives only empty values\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine vegetation cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi = (((data.nir08 - data.red) / (data.red + data.nir08)).\n        resample(time=\"MS\").\n        median(\"time\", keep_attrs=True).\n        compute()\n)\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=4)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fa18b685fc0&gt;"
  },
  {
    "objectID": "tutorials/python/intro-python.html#zonal-statistics",
    "href": "tutorials/python/intro-python.html#zonal-statistics",
    "title": "Exploring the Legacy of Redlining",
    "section": "",
    "text": "In addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\n# make sure we are in the same projection.\n# zonal_stats method understands paths but not xarray\n(   ndvi.\n    rio.reproject(\"EPSG:4326\").\n    rio.to_raster(raster_path=\"ndvi.tif\", driver=\"COG\")   \n)\n\nsf_url = \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\"\nmean_ndvi = zonal_stats(sf_url, \"ndvi.tif\", stats=\"mean\")\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\nsf = gpd.read_file(sf_url)\nsf[\"ndvi\"] = [x[\"mean\"] for x in mean_ndvi]\nsf.plot(column=\"ndvi\", legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nAre historically redlined areas still less green?\n\nimport geopolars as gpl\nimport polars as pl\n\n(gpl.\n  from_geopandas(sf).\n  group_by(\"grade\").\n  agg(pl.col(\"ndvi\").mean()).\n  sort(\"grade\")\n)\n\n\nshape: (5, 2)\n\n\n\ngrade\nndvi\n\n\nstr\nf64\n\n\n\n\nnull\n0.159305\n\n\n\"A\"\n0.337976\n\n\n\"B\"\n0.247661\n\n\n\"C\"\n0.236407\n\n\n\"D\"\n0.232125"
  },
  {
    "objectID": "tutorials/R/2-earthdata.html",
    "href": "tutorials/R/2-earthdata.html",
    "title": "",
    "section": "",
    "text": "The NASA EarthData program provides access to an extensive collection of spatial data products from each of its 12 Distributed Active Archive Centers (‘DAACs’) on the high-performance S3 storage system of Amazon Web Services (AWS). We can take advantage of range requests with NASA EarthData URLs, but unlike the previous examples, NASA requires an authentication step. NASA offers several different mechanisms, including netrc authentication, token-based authentication, and S3 credentials, but only the first of these works equally well from locations both inside and outside of AWS-based compute, so there really is very little reason to learn the other two.\nThe earthdatalogin package in R or the earthaccess package in Python handle the authentication. The R package sets up authentication behind the scenes using environmental variables.\n\nearthdatalogin::edl_netrc()\n\n(A default login is supplied though users are encouraged to register for their own individual accounts.) Once this is in place, EarthData’s protected URLs can be used like any other:\n\nterra::rast(\"https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T56JKT.2023246T235950.v2.0/HLS.L30.T56JKT.2023246T235950.v2.0.SAA.tif\",\n     vsi=TRUE)\n\nclass       : SpatRaster \ndimensions  : 3660, 3660, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 199980, 309780, 7190200, 7300000  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 56N (EPSG:32656) \nsource      : HLS.L30.T56JKT.2023246T235950.v2.0.SAA.tif \nname        : HLS.L30.T56JKT.2023246T235950.v2.0.SAA"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "This project seeks to introduce cloud-native approaches to geospatial analysis in R & Python through the lens of environmental justice applications. This is not meant as a complete course in geospatial analysis – though we encourage interested readers to consider Geocomputation in R or Python as an excellent resource. We present opinionated recipes meant to empower users with the following design goals:\n\nOpen science: open source software, open data, open standards, and reproducibility are emphasized.\nRecipes are presented as reproducible computational notebooks (Quarto and Jupyter) set in narrative analysis.\nAccess to cloud-based compute is not required. The high speed networks of commercial cloud providers can accelerate network-bound operations, but can run from anywhere.\nAccess to a powerful laptop or desktop is not required either. Examples can run from a browser interface on free compute resources such as Codespaces.\nPortable software environments: a Docker container with precise software used to produce these examples. The container is build from the included Dockerfile using the included GitHub Action and is deployed by the Codespaces instance.\nAlignment between R and Python: recipes will try to show approaches in both languages and seek to utilize common interfaces (such as STAC and GDAL) that can be shared across these platforms.\n\nThis is a work in progress! Check back often, and feedback welcome! Test these modules, file an issue or pull request, launch into a Codespaces environment, or reach out on the discussion board.\nWhat is “cloud-native” anyway? We define cloud-native to mean simply that data is accessed over http range request methods, rather than downloading entire files. Code-based examples will develop why this is important and how it differs from renting cloud-based compute. The core philosophy is that what many users already know how to do locally translates pretty seamlessly here, and then a bit extra is required to coerce certain software to stay in ‘range request’ mode and not get greedy trying to download everything. Some authors define this concept somewhat differently, and certainly not all range requests give the same performance, nor are http range requests best in all cases.\n\n\nThanks to input, suggestions, feedback and ideas from so many amazing folks in the OpenScapes community and input and financial support from the NASA TOPS community."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "",
    "section": "",
    "text": "Thanks to input, suggestions, feedback and ideas from so many amazing folks in the OpenScapes community and input and financial support from the NASA TOPS community."
  },
  {
    "objectID": "tutorials/R/1-intro-R.html",
    "href": "tutorials/R/1-intro-R.html",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow in both R and python. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing.\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\nlibrary(dplyr)\ngdalcubes::gdalcubes_options(parallel = TRUE)\n\n\n\n\n\nThe first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox &lt;- c(xmin=-122.51, ymin=37.71, xmax=-122.36, ymax=37.81) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  ext_query(\"eo:cloud_cover\" &lt; 20) |&gt;\n  post_request()\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ncol &lt;- stac_image_collection(items$features, asset_names = c(\"B08\", \"B04\", \"SCL\"))\n\ncube &lt;- cube_view(srs =\"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  dx = 0.0001, dy = 0.0001, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\ndata &lt;-  raster_cube(col, cube, mask = mask)\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine tree cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi &lt;- data |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\n\nndvi_stars &lt;- st_as_stars(ndvi)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nmako &lt;- tm_scale_continuous(values = viridisLite::mako(30))\nfill &lt;- tm_scale_continuous(values = \"Greens\")\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako)\n\n\n\n\n\n\n\nWe examine the present-day impact of historic “red-lining” of US cities during the Great Depression using data from the Mapping Inequality project. All though this racist practice was banned by federal law under the Fair Housing Act of 1968, the systemic scars of that practice are still so deeply etched on our landscape that the remain visible from space – “red-lined” areas (graded “D” under the racist HOLC scheme) show systematically lower greenness than predominately-white neighborhoods (Grade “A”). Trees provide many benefits, from mitigating urban heat to biodiversity, real-estate value, to health.\n\n\nIn addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\nsf &lt;- \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\" |&gt;\n  st_read() |&gt;\n  st_make_valid() |&gt;\n  select(-label_coords)\n\n\npoly &lt;- ndvi |&gt; extract_geom(sf, FUN = mean, reduce_time = TRUE)\nsf$NDVI &lt;- poly$NDVI\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade, using tmap. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako) +\n  tm_shape(sf) + tm_polygons('NDVI', fill.scale = fill) +\n  tm_shape(sf) + tm_text(\"grade\", col=\"darkblue\", size=0.6) +\n  tm_legend_hide()\n\n\n\n\nAre historically redlined areas still less green?\n\nsf |&gt; \n  as_tibble() |&gt;\n  group_by(grade) |&gt; \n  summarise(ndvi = mean(NDVI), \n            sd = sd(NDVI)) |&gt;\n  knitr::kable()\n\n\n\n\ngrade\nndvi\nsd\n\n\n\n\nA\n0.3201204\n0.0611414\n\n\nB\n0.2138501\n0.0783221\n\n\nC\n0.1956334\n0.0564822\n\n\nD\n0.1949736\n0.0385805\n\n\nNA\n0.0962092\nNA"
  },
  {
    "objectID": "tutorials/R/1-intro-R.html#exploring-the-legacy-of-redlining",
    "href": "tutorials/R/1-intro-R.html#exploring-the-legacy-of-redlining",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow in both R and python. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing.\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\nlibrary(dplyr)\ngdalcubes::gdalcubes_options(parallel = TRUE)"
  },
  {
    "objectID": "tutorials/R/1-intro-R.html#data-discovery",
    "href": "tutorials/R/1-intro-R.html#data-discovery",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "The first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox &lt;- c(xmin=-122.51, ymin=37.71, xmax=-122.36, ymax=37.81) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  ext_query(\"eo:cloud_cover\" &lt; 20) |&gt;\n  post_request()\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ncol &lt;- stac_image_collection(items$features, asset_names = c(\"B08\", \"B04\", \"SCL\"))\n\ncube &lt;- cube_view(srs =\"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  dx = 0.0001, dy = 0.0001, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\ndata &lt;-  raster_cube(col, cube, mask = mask)\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine tree cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi &lt;- data |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\n\nndvi_stars &lt;- st_as_stars(ndvi)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nmako &lt;- tm_scale_continuous(values = viridisLite::mako(30))\nfill &lt;- tm_scale_continuous(values = \"Greens\")\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako)"
  },
  {
    "objectID": "tutorials/R/1-intro-R.html#zonal-statistics",
    "href": "tutorials/R/1-intro-R.html#zonal-statistics",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "In addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\nsf &lt;- \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\" |&gt;\n  st_read() |&gt;\n  st_make_valid() |&gt;\n  select(-label_coords)\n\n\npoly &lt;- ndvi |&gt; extract_geom(sf, FUN = mean, reduce_time = TRUE)\nsf$NDVI &lt;- poly$NDVI\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade, using tmap. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako) +\n  tm_shape(sf) + tm_polygons('NDVI', fill.scale = fill) +\n  tm_shape(sf) + tm_text(\"grade\", col=\"darkblue\", size=0.6) +\n  tm_legend_hide()\n\n\n\n\nAre historically redlined areas still less green?\n\nsf |&gt; \n  as_tibble() |&gt;\n  group_by(grade) |&gt; \n  summarise(ndvi = mean(NDVI), \n            sd = sd(NDVI)) |&gt;\n  knitr::kable()\n\n\n\n\ngrade\nndvi\nsd\n\n\n\n\nA\n0.3201204\n0.0611414\n\n\nB\n0.2138501\n0.0783221\n\n\nC\n0.1956334\n0.0564822\n\n\nD\n0.1949736\n0.0385805\n\n\nNA\n0.0962092\nNA"
  },
  {
    "objectID": "tutorials/computing-environment.html",
    "href": "tutorials/computing-environment.html",
    "title": "",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud.\n\n\n\nEach of the recipes on this site correspond to a Quarto or Jupyter notebook in a GitHub repository (see contents/ directory or footer links). Such notebooks form the basis of technical documentation and publishing to a wide array of formats. A _quarto.yml configuration file in the repository root determines the website layout. Notebooks can be run interactively in any appropriate environment (RStudio, JupyterLab, VSCode, etc, see below for free online platforms). Type quarto render from the bash command line or use quarto::quarto_preview() from the R console to preview the entire site.\n\n\n\n\n\n\n \nBoth GitHub Codespaces and Gitpod provide a fast and simple way to enter into integrated development environments such as VSCode, RStudio, or JupyterLab on free, cloud-based virtual machines. Codespaces has a free tier of 60 hours/month, Gitpod of 50 hours a month, both offer paid plans for additional use and larger compute instances. Small codespace instances are also free to instructors\nBy clicking on one of the buttons in GitHub, users will be placed into a free cloud-based virtual machine running a VSCode editor in their web browser. The first setup can take a few minutes to complete.\nAdditionally, this also provides access to an RStudio environment on an embedded port for users who prefer that editor to VSCode. Once the Codespace has fully completed loading, it will include a link in a second welcome message in the Terminal to access RStudio like so:\n\nThe RStudio link can also always be accessed from the Ports tab under the port labeled “Rstudio” (8787). (Gitpod will show a pop-up message to open this port instead.)\nBoth Codespaces and Gitpod can be configured with custom compute environments by supplying a docker image. Both the VSCode and RStudio editors run in the same underlying custom Docker container. This repository includes a Dockerfile defining this compute environment which includes specific versions of R and python packages, the latest releases of the OSGeo C libraries GDAL, PROJ, and GEOS that power many spatial operations in both languages. These elements are pre-compiled in a Docker container based on the latest Ubuntu LTS release (22.04 at the time of writing), which itself is build according to the Dockerfile found in this repository using a GitHub Action. The devcontainer.json configuration will also set up relevant VSCode extensions for both both Jupyter and Quarto notebooks, with each supporting both R and Python.\n\n\n\nOpen this repository in a local Visual Studio Code editor on a Mac, Linux, or Windows laptop and you will probably be prompted “Do you want to open this in project in a Dev Container?” If you agree, VSCode will attempt to use a local Docker installation to pull a container with much of the required software already installed. This uses the same Docker container and enables all the same extensions in VSCode, including RStudio server on the embedded port.\nOf course, users can open this project in a local VSCode or any other favorite editor without opening in the devcontainer. The user assumes responsibility to install necessary software, i.e. the packages listed in requirements.txt or install.R. Note that doing so does not ensure that the same version of system libraries like GDAL, PROJ, or GEOS will necessarily be used. For most operations this should not matter, but users on older versions of GDAL may encounter worse performance or other difficulties.\n\n\n\nWe can sidesteps elements specific to the VSCode editor defined in the devcontainer.json configuration while still leveraging the same system libraries and pre-built packages. For example, a user could also choose to run (or extend) the underlying docker container independently, e.g.\ndocker run --rm -ti ghcr.io/boettiger-lab/nasa-tops:latest bash\nwhich opens a bash terminal inside the container. This approach is also compatible with most HPC setups using singularity instead of docker.\nSome users may not be familiar with editing and running code entirely from a bash shell, so the container also includes RStudio server and thus can be run to launch RStudio in an open port instead,\ndocker run -d -p 8787:8787 --user root -e DISABLE_AUTH=true \\\n  ghcr.io/boettiger-lab/nasa-tops:latest\nand visit http://localhost:8787 to connect."
  },
  {
    "objectID": "tutorials/computing-environment.html#portable-reproducibility",
    "href": "tutorials/computing-environment.html#portable-reproducibility",
    "title": "",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud."
  },
  {
    "objectID": "tutorials/computing-environment.html#notebooks-on-github",
    "href": "tutorials/computing-environment.html#notebooks-on-github",
    "title": "",
    "section": "",
    "text": "Each of the recipes on this site correspond to a Quarto or Jupyter notebook in a GitHub repository (see contents/ directory or footer links). Such notebooks form the basis of technical documentation and publishing to a wide array of formats. A _quarto.yml configuration file in the repository root determines the website layout. Notebooks can be run interactively in any appropriate environment (RStudio, JupyterLab, VSCode, etc, see below for free online platforms). Type quarto render from the bash command line or use quarto::quarto_preview() from the R console to preview the entire site."
  },
  {
    "objectID": "tutorials/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "href": "tutorials/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "title": "",
    "section": "",
    "text": "Both GitHub Codespaces and Gitpod provide a fast and simple way to enter into integrated development environments such as VSCode, RStudio, or JupyterLab on free, cloud-based virtual machines. Codespaces has a free tier of 60 hours/month, Gitpod of 50 hours a month, both offer paid plans for additional use and larger compute instances. Small codespace instances are also free to instructors\nBy clicking on one of the buttons in GitHub, users will be placed into a free cloud-based virtual machine running a VSCode editor in their web browser. The first setup can take a few minutes to complete.\nAdditionally, this also provides access to an RStudio environment on an embedded port for users who prefer that editor to VSCode. Once the Codespace has fully completed loading, it will include a link in a second welcome message in the Terminal to access RStudio like so:\n\nThe RStudio link can also always be accessed from the Ports tab under the port labeled “Rstudio” (8787). (Gitpod will show a pop-up message to open this port instead.)\nBoth Codespaces and Gitpod can be configured with custom compute environments by supplying a docker image. Both the VSCode and RStudio editors run in the same underlying custom Docker container. This repository includes a Dockerfile defining this compute environment which includes specific versions of R and python packages, the latest releases of the OSGeo C libraries GDAL, PROJ, and GEOS that power many spatial operations in both languages. These elements are pre-compiled in a Docker container based on the latest Ubuntu LTS release (22.04 at the time of writing), which itself is build according to the Dockerfile found in this repository using a GitHub Action. The devcontainer.json configuration will also set up relevant VSCode extensions for both both Jupyter and Quarto notebooks, with each supporting both R and Python."
  },
  {
    "objectID": "tutorials/computing-environment.html#locally-vscode",
    "href": "tutorials/computing-environment.html#locally-vscode",
    "title": "",
    "section": "",
    "text": "Open this repository in a local Visual Studio Code editor on a Mac, Linux, or Windows laptop and you will probably be prompted “Do you want to open this in project in a Dev Container?” If you agree, VSCode will attempt to use a local Docker installation to pull a container with much of the required software already installed. This uses the same Docker container and enables all the same extensions in VSCode, including RStudio server on the embedded port.\nOf course, users can open this project in a local VSCode or any other favorite editor without opening in the devcontainer. The user assumes responsibility to install necessary software, i.e. the packages listed in requirements.txt or install.R. Note that doing so does not ensure that the same version of system libraries like GDAL, PROJ, or GEOS will necessarily be used. For most operations this should not matter, but users on older versions of GDAL may encounter worse performance or other difficulties."
  },
  {
    "objectID": "tutorials/computing-environment.html#anywhere-docker",
    "href": "tutorials/computing-environment.html#anywhere-docker",
    "title": "",
    "section": "",
    "text": "We can sidesteps elements specific to the VSCode editor defined in the devcontainer.json configuration while still leveraging the same system libraries and pre-built packages. For example, a user could also choose to run (or extend) the underlying docker container independently, e.g.\ndocker run --rm -ti ghcr.io/boettiger-lab/nasa-tops:latest bash\nwhich opens a bash terminal inside the container. This approach is also compatible with most HPC setups using singularity instead of docker.\nSome users may not be familiar with editing and running code entirely from a bash shell, so the container also includes RStudio server and thus can be run to launch RStudio in an open port instead,\ndocker run -d -p 8787:8787 --user root -e DISABLE_AUTH=true \\\n  ghcr.io/boettiger-lab/nasa-tops:latest\nand visit http://localhost:8787 to connect."
  }
]