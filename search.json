[
  {
    "objectID": "tutorials/python/2-NASA-EarthData.html",
    "href": "tutorials/python/2-NASA-EarthData.html",
    "title": "Cloud-native EarthData Access",
    "section": "",
    "text": "We show how to run existing NASA data workflows on the cloud, in parallel, without a cloud service account using your laptop.\nNASA recently announced completion of the transfer of some 59 petabytes of data to the Amazon cloud – a core component of NASA’s Transformation to Open Science (TOPS) mission. Researchers are frequently told that to take advantage of such “cloud data” they will need to pay (or find a grant or other program to pay) for cloud computing resources. This approach is sometimes describes as “send the compute to the data”. While this narrative is no doubt beneficial to Amazon Inc, it exacerbates inequity and is often misleading. The purpose of having data in a cloud storage platform is not just to make it faster to access that data on rented cloud computing platforms. The high bandwith and high disk speeds provided by these systems can be just as powerful when you provide your own compute. Consistent with NASA’s vision, this means that high-performance access is free\n\nNASA Earth science data have been freely openly and available to all users since EOSDIS became operational in 1994. Under NASA’s full and open data policy, all NASA mission data (along with the algorithms, metadata, and documentation associated with these data) must be freely available to the public. This means that anyone, anywhere in the world, can access the more than 59 PB of NASA Earth science data without restriction\n\nAll we need is software that can treat the cloud storage as if it were local storage: a virtual filesystem. The ability to do this – the HTTP range request standard – has been around for over two decades and is widely implemented in open source software. Unfortunately, many users and workflows are stuck in an old model that assumes individual files must always be downloaded first. We even see this in workflows designed to run on cloud platforms. A recent blog post by commercial platform Coiled provides a typical illustration of this. Typical of most workflows, the example relies on only a small subset of the data found in each file (specific spatial region and specific variables), but uses an approach that downloads all 500+ GB of files anyway. A faster approach will only download the range of data that we actually use. The Coiled blog post shows a full download-based workflow that takes 6.4hrs to complete locally, and then argues that by adding some extra code and paying Amazon for compute time, the same workflow can run much faster. Here we will consider the same workflow again, but this time make it much faster just by removing the download loop and running it from a local machine for free. We also compare a second way that dispatches via a different virtual filesystem approach and is even faster.\n\nimport earthaccess\nimport rioxarray\nimport rasterio\nimport xarray as xr\nimport numpy as np\n\nUSER NOTE: This example processes daily data files over 2 years – 729 netcdf files in all. To make this example run more quickly for testing purposes, merely shorten the temporal span of data requested here!\n\nresults = earthaccess.search_data(\n    short_name=\"MUR-JPL-L4-GLOB-v4.1\",\n    temporal=(\"2020-01-01\", \"2021-12-31\"),\n)\n\nGranules found: 729\n\n\n\n\nRather than tediously downloading the entirety of each file and then manually looping over each one to open, read, and concatinate as shown in the Coiled Demo, we can simply open the whole set in one go. This lazy read method allows us to then range-request only the subset of data we need from each file, thanks to earthaccess using a cloud-native reads over http via the python fsspec package. We can then issue the usual xarray operations to process and plot the data, treating the remote source as if it were already sitting on our local disk. This approach is quite fast, works from any machine, and does not require spending money on AWS. (note that fsspec package, which is doing the core magic of allowing us to treat the remote filesystem as if it were a local filesystem, is not explicitly visible in this workflow, but earthaccess has taken care of it for us).\nNote this code is also simpler and more concise than the implementation shown in download-based workflow. Setting up the fsspec connections takes about 29 minutes. because the evaluation is lazy, most of the computation only occurs at the last step, when we create the plot, which takes about 46 minutes.\n\n%%time\nfiles = earthaccess.open(results)\nds = xr.open_mfdataset(files,\n                       decode_times=False, \n                       data_vars=['analysed_sst', 'sea_ice_fraction'], \n                       concat_dim=\"time\", \n                       combine=\"nested\",\n                       parallel=True)\n\nOpening 729 granules, approx size: 505.34 GB\nCPU times: user 2min 12s, sys: 35.9 s, total: 2min 48s\nWall time: 29min 35s\n\n\n\n\n\n\n\n\n\n\n\n\n%%time\ndds = ds.sel(lon=slice(-93, -76), lat=slice(41, 49))\ncond = (dds.sea_ice_fraction &lt; 0.15) | np.isnan(dds.sea_ice_fraction)\nresult = dds.analysed_sst.where(cond)\n\nCPU times: user 21.3 ms, sys: 0 ns, total: 21.3 ms\nWall time: 19.4 ms\n\n\n\n%%time\nresult.std(\"time\").plot(figsize=(14, 6), x=\"lon\", y=\"lat\")\n\n/home/jovyan/.virtualenvs/spatial/lib/python3.10/site-packages/dask/array/numpy_compat.py:51: RuntimeWarning: invalid value encountered in divide\n  x = np.divide(x1, x2, out)\n\n\nCPU times: user 3min 51s, sys: 1min 28s, total: 5min 19s\nWall time: 46min 19s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f4dfd91b370&gt;\n\n\n\n\n\n\n\n\nA different virtual filesystem approach is available through GDAL. While fsspec tries to provide a generic POSIX-like interface to remote files, the GDAL VSI is specifically optimized for spatial data and often considerably faster. The rioxarray package provides a drop-in engine to xarray’s open_mfdataset that uses GDAL. (Aside – at least some of the other netcdf engines supported by xarray should also be able to natively perform range requests over URLs to data without needing the fsspec layer added by earthaccess, and may have better performance. This case is not illustrated in this notebook). Here we’ll use the GDAL VSI.\nBecause the NASA EarthData are behind a security layer, using the URLs directly instead of earthaccess with fsspec requires a little extra handling of authentication process to make GDAL aware of the NETRC and cookie files it needs. We’ll also set some of the optional but recommended options for GDAL when using the virtual filesystem. Unfortunately this makes our code look a bit verbose – ideally packages like rioxarray would take care of these things.\nNote the GDAL is about 3x faster at setting up the virtual filesystem, and a little faster in the xarray/dask dispatch to compute the plot. (When this approach is combined with metadata from a STAC catalog, it does not need to read individual file metadata and the first step can become almost instant). GDAL performance is constantly improving, especially with regards to cloud native reads, so a recent version can make a huge difference.\n\nrasterio.show_versions()\n\nrasterio info:\n  rasterio: 1.3.9\n      GDAL: 3.6.4\n      PROJ: 9.0.1\n      GEOS: 3.11.1\n PROJ DATA: /opt/venv/lib/python3.10/site-packages/rasterio/proj_data\n GDAL DATA: /opt/venv/lib/python3.10/site-packages/rasterio/gdal_data\n\nSystem:\n    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\nexecutable: /opt/venv/bin/python\n   machine: Linux-6.6.10-76060610-generic-x86_64-with-glibc2.35\n\nPython deps:\n    affine: 2.4.0\n     attrs: 23.2.0\n   certifi: 2024.02.02\n     click: 8.1.7\n     cligj: 0.7.2\n    cython: None\n     numpy: 1.26.3\n    snuggs: 1.4.7\nclick-plugins: None\nsetuptools: 59.6.0\n\n\n\n%%time\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom pathlib import Path\ncookies = os.path.expanduser(\"~/.urs_cookies\")\nPath(cookies).touch()\n\n## pull out the URLs\ndata_links = [granule.data_links(access=\"external\") for granule in results]\nurl_links = [f'{link[0]}' for link in data_links]\n\n\n# and here we go\nwith rasterio.Env(GDAL_INGESTED_BYTES_AT_OPEN=\"32000\",\n                  GDAL_HTTP_MULTIPLEX=\"YES\",\n                  GDAL_HTTP_MERGE_CONSECUTIVE_RANGES=\"YES\",\n                  GDAL_HTTP_VERSION=\"2\",\n                  GDAL_NUM_THREADS=\"ALL_CPUS\",\n                  GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n                  GDAL_HTTP_COOKIEFILE=cookies, \n                  GDAL_HTTP_COOKIEJAR=cookies, \n                  GDAL_HTTP_NETRC=True):\n    ds1 = xr.open_mfdataset(url_links, \n                           engine = \"rasterio\", \n                           concat_dim=\"time\", \n                           combine=\"nested\",\n                           )\n\nCPU times: user 47.7 s, sys: 3.78 s, total: 51.4 s\nWall time: 11min 48s\n\n\n\n%%time\ndds = ds1.sel(x=slice(18000-9300, 18000-7600), y = slice(9000+4100,9000+4900))\ndds.analysed_sst.std(\"time\").plot(figsize=(14, 6), x=\"x\", y=\"y\")\n\nCPU times: user 2min 13s, sys: 3min 13s, total: 5min 27s\nWall time: 32min 55s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f4cf8396b30&gt;\n\n\n\n\n\n\n\n\nThe GDAL VSI is already widely used under the hood by python packages working with cloud-optimized geotiff (COG) files (e.g. via odc.stac, which like the above approach also produces dask-backed xarrays), and also widely used by most other languages (e.g. R) for working with any spatial data. To GDAL, netcdf and other so-called “n-dimensional array” formats like h5, zarr are just a handful of the 160-odd formats of “raster” data it supports, along with formats like COG and GeoTIFF files. It can be particularly powerful in more complicated workflows which require spatially-aware operations such as reprojection and aggregation. The GDAL VSI can sometimes be considerably faster than fsspec, expecially when configured for cloud-native access. The nusiance of these environmental variables aside, it can also be considerably easier to use and to generalize patterns across data formats (netcdf, zarr, COG), and across languages (R, C++, javascript, julia etc), since GDAL understands [all these formats] and is used in all of these languages, as well as in platforms such as Google Earth Engine and QGIS. This makes it a natural bridge between languages. This broad use over decades has made GDAL very powerful, and it continues to improve rapidly with frequent releases.\nGDAL is not just for COGs. The python ecosystem has a rich set of patterns for range-request reads of Cloud Optimized Geotif (COG) files using packages like odc.stac, as illustrated in our intro to cloud-native python. But possibly for historical/cultural reasons, at present the python geospatial community seems to prefer to access ncdf and similar n-dimensional-array formats without GDAL, whether by relying on downloading complete files, using fsspec, or other dedicated libraries (zarr). There are possibly many reasons for this. One is a divide between the the “Geospatial Information Systems” community, that thinks of file serializations as “rasters” or “vectors”, and the “modeler” community, which thinks of data as “n-dimensional arrays”. Both have their weaknesses and the lines are frequently blurred, but one obvious manifestation is in how each one writes their netcdf files (and how much they rely on GDAL). For instance, this NASA product, strongly centered in the modeler community is sometimes sloppy about these metadata conventions, and as a result GDAL (especially older versions), might not detect all the details appropriately. Note that GDAL has failed to recognize the units of lat-long, so we have had to subset the x-y positions manually."
  },
  {
    "objectID": "tutorials/python/2-NASA-EarthData.html#using-fsspec-virtual-filesystem-via-earthacess",
    "href": "tutorials/python/2-NASA-EarthData.html#using-fsspec-virtual-filesystem-via-earthacess",
    "title": "Cloud-native EarthData Access",
    "section": "",
    "text": "Rather than tediously downloading the entirety of each file and then manually looping over each one to open, read, and concatinate as shown in the Coiled Demo, we can simply open the whole set in one go. This lazy read method allows us to then range-request only the subset of data we need from each file, thanks to earthaccess using a cloud-native reads over http via the python fsspec package. We can then issue the usual xarray operations to process and plot the data, treating the remote source as if it were already sitting on our local disk. This approach is quite fast, works from any machine, and does not require spending money on AWS. (note that fsspec package, which is doing the core magic of allowing us to treat the remote filesystem as if it were a local filesystem, is not explicitly visible in this workflow, but earthaccess has taken care of it for us).\nNote this code is also simpler and more concise than the implementation shown in download-based workflow. Setting up the fsspec connections takes about 29 minutes. because the evaluation is lazy, most of the computation only occurs at the last step, when we create the plot, which takes about 46 minutes.\n\n%%time\nfiles = earthaccess.open(results)\nds = xr.open_mfdataset(files,\n                       decode_times=False, \n                       data_vars=['analysed_sst', 'sea_ice_fraction'], \n                       concat_dim=\"time\", \n                       combine=\"nested\",\n                       parallel=True)\n\nOpening 729 granules, approx size: 505.34 GB\nCPU times: user 2min 12s, sys: 35.9 s, total: 2min 48s\nWall time: 29min 35s\n\n\n\n\n\n\n\n\n\n\n\n\n%%time\ndds = ds.sel(lon=slice(-93, -76), lat=slice(41, 49))\ncond = (dds.sea_ice_fraction &lt; 0.15) | np.isnan(dds.sea_ice_fraction)\nresult = dds.analysed_sst.where(cond)\n\nCPU times: user 21.3 ms, sys: 0 ns, total: 21.3 ms\nWall time: 19.4 ms\n\n\n\n%%time\nresult.std(\"time\").plot(figsize=(14, 6), x=\"lon\", y=\"lat\")\n\n/home/jovyan/.virtualenvs/spatial/lib/python3.10/site-packages/dask/array/numpy_compat.py:51: RuntimeWarning: invalid value encountered in divide\n  x = np.divide(x1, x2, out)\n\n\nCPU times: user 3min 51s, sys: 1min 28s, total: 5min 19s\nWall time: 46min 19s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f4dfd91b370&gt;"
  },
  {
    "objectID": "tutorials/python/2-NASA-EarthData.html#using-gdal-virtual-filesystem",
    "href": "tutorials/python/2-NASA-EarthData.html#using-gdal-virtual-filesystem",
    "title": "Cloud-native EarthData Access",
    "section": "",
    "text": "A different virtual filesystem approach is available through GDAL. While fsspec tries to provide a generic POSIX-like interface to remote files, the GDAL VSI is specifically optimized for spatial data and often considerably faster. The rioxarray package provides a drop-in engine to xarray’s open_mfdataset that uses GDAL. (Aside – at least some of the other netcdf engines supported by xarray should also be able to natively perform range requests over URLs to data without needing the fsspec layer added by earthaccess, and may have better performance. This case is not illustrated in this notebook). Here we’ll use the GDAL VSI.\nBecause the NASA EarthData are behind a security layer, using the URLs directly instead of earthaccess with fsspec requires a little extra handling of authentication process to make GDAL aware of the NETRC and cookie files it needs. We’ll also set some of the optional but recommended options for GDAL when using the virtual filesystem. Unfortunately this makes our code look a bit verbose – ideally packages like rioxarray would take care of these things.\nNote the GDAL is about 3x faster at setting up the virtual filesystem, and a little faster in the xarray/dask dispatch to compute the plot. (When this approach is combined with metadata from a STAC catalog, it does not need to read individual file metadata and the first step can become almost instant). GDAL performance is constantly improving, especially with regards to cloud native reads, so a recent version can make a huge difference.\n\nrasterio.show_versions()\n\nrasterio info:\n  rasterio: 1.3.9\n      GDAL: 3.6.4\n      PROJ: 9.0.1\n      GEOS: 3.11.1\n PROJ DATA: /opt/venv/lib/python3.10/site-packages/rasterio/proj_data\n GDAL DATA: /opt/venv/lib/python3.10/site-packages/rasterio/gdal_data\n\nSystem:\n    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\nexecutable: /opt/venv/bin/python\n   machine: Linux-6.6.10-76060610-generic-x86_64-with-glibc2.35\n\nPython deps:\n    affine: 2.4.0\n     attrs: 23.2.0\n   certifi: 2024.02.02\n     click: 8.1.7\n     cligj: 0.7.2\n    cython: None\n     numpy: 1.26.3\n    snuggs: 1.4.7\nclick-plugins: None\nsetuptools: 59.6.0\n\n\n\n%%time\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom pathlib import Path\ncookies = os.path.expanduser(\"~/.urs_cookies\")\nPath(cookies).touch()\n\n## pull out the URLs\ndata_links = [granule.data_links(access=\"external\") for granule in results]\nurl_links = [f'{link[0]}' for link in data_links]\n\n\n# and here we go\nwith rasterio.Env(GDAL_INGESTED_BYTES_AT_OPEN=\"32000\",\n                  GDAL_HTTP_MULTIPLEX=\"YES\",\n                  GDAL_HTTP_MERGE_CONSECUTIVE_RANGES=\"YES\",\n                  GDAL_HTTP_VERSION=\"2\",\n                  GDAL_NUM_THREADS=\"ALL_CPUS\",\n                  GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n                  GDAL_HTTP_COOKIEFILE=cookies, \n                  GDAL_HTTP_COOKIEJAR=cookies, \n                  GDAL_HTTP_NETRC=True):\n    ds1 = xr.open_mfdataset(url_links, \n                           engine = \"rasterio\", \n                           concat_dim=\"time\", \n                           combine=\"nested\",\n                           )\n\nCPU times: user 47.7 s, sys: 3.78 s, total: 51.4 s\nWall time: 11min 48s\n\n\n\n%%time\ndds = ds1.sel(x=slice(18000-9300, 18000-7600), y = slice(9000+4100,9000+4900))\ndds.analysed_sst.std(\"time\").plot(figsize=(14, 6), x=\"x\", y=\"y\")\n\nCPU times: user 2min 13s, sys: 3min 13s, total: 5min 27s\nWall time: 32min 55s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f4cf8396b30&gt;"
  },
  {
    "objectID": "tutorials/python/2-NASA-EarthData.html#comparisons",
    "href": "tutorials/python/2-NASA-EarthData.html#comparisons",
    "title": "Cloud-native EarthData Access",
    "section": "",
    "text": "The GDAL VSI is already widely used under the hood by python packages working with cloud-optimized geotiff (COG) files (e.g. via odc.stac, which like the above approach also produces dask-backed xarrays), and also widely used by most other languages (e.g. R) for working with any spatial data. To GDAL, netcdf and other so-called “n-dimensional array” formats like h5, zarr are just a handful of the 160-odd formats of “raster” data it supports, along with formats like COG and GeoTIFF files. It can be particularly powerful in more complicated workflows which require spatially-aware operations such as reprojection and aggregation. The GDAL VSI can sometimes be considerably faster than fsspec, expecially when configured for cloud-native access. The nusiance of these environmental variables aside, it can also be considerably easier to use and to generalize patterns across data formats (netcdf, zarr, COG), and across languages (R, C++, javascript, julia etc), since GDAL understands [all these formats] and is used in all of these languages, as well as in platforms such as Google Earth Engine and QGIS. This makes it a natural bridge between languages. This broad use over decades has made GDAL very powerful, and it continues to improve rapidly with frequent releases.\nGDAL is not just for COGs. The python ecosystem has a rich set of patterns for range-request reads of Cloud Optimized Geotif (COG) files using packages like odc.stac, as illustrated in our intro to cloud-native python. But possibly for historical/cultural reasons, at present the python geospatial community seems to prefer to access ncdf and similar n-dimensional-array formats without GDAL, whether by relying on downloading complete files, using fsspec, or other dedicated libraries (zarr). There are possibly many reasons for this. One is a divide between the the “Geospatial Information Systems” community, that thinks of file serializations as “rasters” or “vectors”, and the “modeler” community, which thinks of data as “n-dimensional arrays”. Both have their weaknesses and the lines are frequently blurred, but one obvious manifestation is in how each one writes their netcdf files (and how much they rely on GDAL). For instance, this NASA product, strongly centered in the modeler community is sometimes sloppy about these metadata conventions, and as a result GDAL (especially older versions), might not detect all the details appropriately. Note that GDAL has failed to recognize the units of lat-long, so we have had to subset the x-y positions manually."
  },
  {
    "objectID": "tutorials/computing-environment.html",
    "href": "tutorials/computing-environment.html",
    "title": "",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud.\n\n\n\nEach of the recipes on this site correspond to a Quarto or Jupyter notebook in a GitHub repository (see contents/ directory or footer links). Such notebooks form the basis of technical documentation and publishing to a wide array of formats. A _quarto.yml configuration file in the repository root determines the website layout. Notebooks can be run interactively in any appropriate environment (RStudio, JupyterLab, VSCode, etc, see below for free online platforms). Type quarto render from the bash command line or use quarto::quarto_preview() from the R console to preview the entire site.\n\n\n\n\n\n\n \nBoth GitHub Codespaces and Gitpod provide a fast and simple way to enter into integrated development environments such as VSCode, RStudio, or JupyterLab on free, cloud-based virtual machines. Codespaces has a free tier of 60 hours/month, Gitpod of 50 hours a month, both offer paid plans for additional use and larger compute instances. Small codespace instances are also free to instructors\nBy clicking on one of the buttons in GitHub, users will be placed into a free cloud-based virtual machine running a VSCode editor in their web browser. The first setup can take a few minutes to complete.\nAdditionally, this also provides access to an RStudio environment on an embedded port for users who prefer that editor to VSCode. Once the Codespace has fully completed loading, it will include a link in a second welcome message in the Terminal to access RStudio like so:\n\nThe RStudio link can also always be accessed from the Ports tab under the port labeled “Rstudio” (8787). (Gitpod will show a pop-up message to open this port instead.)\nBoth Codespaces and Gitpod can be configured with custom compute environments by supplying a docker image. Both the VSCode and RStudio editors run in the same underlying custom Docker container. This repository includes a Dockerfile defining this compute environment which includes specific versions of R and python packages, the latest releases of the OSGeo C libraries GDAL, PROJ, and GEOS that power many spatial operations in both languages. These elements are pre-compiled in a Docker container based on the latest Ubuntu LTS release (22.04 at the time of writing), which itself is build according to the Dockerfile found in this repository using a GitHub Action. The devcontainer.json configuration will also set up relevant VSCode extensions for both both Jupyter and Quarto notebooks, with each supporting both R and Python.\n\n\n\nOpen this repository in a local Visual Studio Code editor on a Mac, Linux, or Windows laptop and you will probably be prompted “Do you want to open this in project in a Dev Container?” If you agree, VSCode will attempt to use a local Docker installation to pull a container with much of the required software already installed. This uses the same Docker container and enables all the same extensions in VSCode, including RStudio server on the embedded port.\nOf course, users can open this project in a local VSCode or any other favorite editor without opening in the devcontainer. The user assumes responsibility to install necessary software, i.e. the packages listed in requirements.txt or install.R. Note that doing so does not ensure that the same version of system libraries like GDAL, PROJ, or GEOS will necessarily be used. For most operations this should not matter, but users on older versions of GDAL may encounter worse performance or other difficulties.\n\n\n\nWe can sidesteps elements specific to the VSCode editor defined in the devcontainer.json configuration while still leveraging the same system libraries and pre-built packages. For example, a user could also choose to run (or extend) the underlying docker container independently, e.g.\ndocker run --rm -ti ghcr.io/boettiger-lab/nasa-tops:latest bash\nwhich opens a bash terminal inside the container. This approach is also compatible with most HPC setups using singularity instead of docker.\nSome users may not be familiar with editing and running code entirely from a bash shell, so the container also includes RStudio server and thus can be run to launch RStudio in an open port instead,\ndocker run -d -p 8787:8787 --user root -e DISABLE_AUTH=true \\\n  ghcr.io/boettiger-lab/nasa-tops:latest\nand visit http://localhost:8787 to connect."
  },
  {
    "objectID": "tutorials/computing-environment.html#portable-reproducibility",
    "href": "tutorials/computing-environment.html#portable-reproducibility",
    "title": "",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud."
  },
  {
    "objectID": "tutorials/computing-environment.html#notebooks-on-github",
    "href": "tutorials/computing-environment.html#notebooks-on-github",
    "title": "",
    "section": "",
    "text": "Each of the recipes on this site correspond to a Quarto or Jupyter notebook in a GitHub repository (see contents/ directory or footer links). Such notebooks form the basis of technical documentation and publishing to a wide array of formats. A _quarto.yml configuration file in the repository root determines the website layout. Notebooks can be run interactively in any appropriate environment (RStudio, JupyterLab, VSCode, etc, see below for free online platforms). Type quarto render from the bash command line or use quarto::quarto_preview() from the R console to preview the entire site."
  },
  {
    "objectID": "tutorials/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "href": "tutorials/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "title": "",
    "section": "",
    "text": "Both GitHub Codespaces and Gitpod provide a fast and simple way to enter into integrated development environments such as VSCode, RStudio, or JupyterLab on free, cloud-based virtual machines. Codespaces has a free tier of 60 hours/month, Gitpod of 50 hours a month, both offer paid plans for additional use and larger compute instances. Small codespace instances are also free to instructors\nBy clicking on one of the buttons in GitHub, users will be placed into a free cloud-based virtual machine running a VSCode editor in their web browser. The first setup can take a few minutes to complete.\nAdditionally, this also provides access to an RStudio environment on an embedded port for users who prefer that editor to VSCode. Once the Codespace has fully completed loading, it will include a link in a second welcome message in the Terminal to access RStudio like so:\n\nThe RStudio link can also always be accessed from the Ports tab under the port labeled “Rstudio” (8787). (Gitpod will show a pop-up message to open this port instead.)\nBoth Codespaces and Gitpod can be configured with custom compute environments by supplying a docker image. Both the VSCode and RStudio editors run in the same underlying custom Docker container. This repository includes a Dockerfile defining this compute environment which includes specific versions of R and python packages, the latest releases of the OSGeo C libraries GDAL, PROJ, and GEOS that power many spatial operations in both languages. These elements are pre-compiled in a Docker container based on the latest Ubuntu LTS release (22.04 at the time of writing), which itself is build according to the Dockerfile found in this repository using a GitHub Action. The devcontainer.json configuration will also set up relevant VSCode extensions for both both Jupyter and Quarto notebooks, with each supporting both R and Python."
  },
  {
    "objectID": "tutorials/computing-environment.html#locally-vscode",
    "href": "tutorials/computing-environment.html#locally-vscode",
    "title": "",
    "section": "",
    "text": "Open this repository in a local Visual Studio Code editor on a Mac, Linux, or Windows laptop and you will probably be prompted “Do you want to open this in project in a Dev Container?” If you agree, VSCode will attempt to use a local Docker installation to pull a container with much of the required software already installed. This uses the same Docker container and enables all the same extensions in VSCode, including RStudio server on the embedded port.\nOf course, users can open this project in a local VSCode or any other favorite editor without opening in the devcontainer. The user assumes responsibility to install necessary software, i.e. the packages listed in requirements.txt or install.R. Note that doing so does not ensure that the same version of system libraries like GDAL, PROJ, or GEOS will necessarily be used. For most operations this should not matter, but users on older versions of GDAL may encounter worse performance or other difficulties."
  },
  {
    "objectID": "tutorials/computing-environment.html#anywhere-docker",
    "href": "tutorials/computing-environment.html#anywhere-docker",
    "title": "",
    "section": "",
    "text": "We can sidesteps elements specific to the VSCode editor defined in the devcontainer.json configuration while still leveraging the same system libraries and pre-built packages. For example, a user could also choose to run (or extend) the underlying docker container independently, e.g.\ndocker run --rm -ti ghcr.io/boettiger-lab/nasa-tops:latest bash\nwhich opens a bash terminal inside the container. This approach is also compatible with most HPC setups using singularity instead of docker.\nSome users may not be familiar with editing and running code entirely from a bash shell, so the container also includes RStudio server and thus can be run to launch RStudio in an open port instead,\ndocker run -d -p 8787:8787 --user root -e DISABLE_AUTH=true \\\n  ghcr.io/boettiger-lab/nasa-tops:latest\nand visit http://localhost:8787 to connect."
  },
  {
    "objectID": "tutorials/R/1-intro-R.html",
    "href": "tutorials/R/1-intro-R.html",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow in both R and python. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing.\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\nlibrary(dplyr)\ngdalcubes::gdalcubes_options(parallel = TRUE)\n\n\n\n\n\nThe first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox &lt;- c(xmin=-122.51, ymin=37.71, xmax=-122.36, ymax=37.81) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  ext_query(\"eo:cloud_cover\" &lt; 20) |&gt;\n  post_request()\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ncol &lt;- stac_image_collection(items$features, asset_names = c(\"B08\", \"B04\", \"SCL\"))\n\ncube &lt;- cube_view(srs =\"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  dx = 0.0001, dy = 0.0001, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\ndata &lt;-  raster_cube(col, cube, mask = mask)\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine tree cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi &lt;- data |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\n\nndvi_stars &lt;- st_as_stars(ndvi)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nmako &lt;- tm_scale_continuous(values = viridisLite::mako(30))\nfill &lt;- tm_scale_continuous(values = \"Greens\")\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako)\n\n\n\n\n\n\n\nWe examine the present-day impact of historic “red-lining” of US cities during the Great Depression using data from the Mapping Inequality project. All though this racist practice was banned by federal law under the Fair Housing Act of 1968, the systemic scars of that practice are still so deeply etched on our landscape that the remain visible from space – “red-lined” areas (graded “D” under the racist HOLC scheme) show systematically lower greenness than predominately-white neighborhoods (Grade “A”). Trees provide many benefits, from mitigating urban heat to biodiversity, real-estate value, to health.\n\n\nIn addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\nsf &lt;- \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\" |&gt;\n  st_read() |&gt;\n  st_make_valid() |&gt;\n  select(-label_coords)\n\n\npoly &lt;- ndvi |&gt; extract_geom(sf, FUN = mean, reduce_time = TRUE)\nsf$NDVI &lt;- poly$NDVI\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade, using tmap. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako) +\n  tm_shape(sf) + tm_polygons('NDVI', fill.scale = fill) +\n  tm_shape(sf) + tm_text(\"grade\", col=\"darkblue\", size=0.6) +\n  tm_legend_hide()\n\n\n\n\nAre historically redlined areas still less green?\n\nsf |&gt; \n  as_tibble() |&gt;\n  group_by(grade) |&gt; \n  summarise(ndvi = mean(NDVI), \n            sd = sd(NDVI)) |&gt;\n  knitr::kable()\n\n\n\n\ngrade\nndvi\nsd\n\n\n\n\nA\n0.3201204\n0.0611414\n\n\nB\n0.2138501\n0.0783221\n\n\nC\n0.1956334\n0.0564822\n\n\nD\n0.1949736\n0.0385805\n\n\nNA\n0.0962092\nNA"
  },
  {
    "objectID": "tutorials/R/1-intro-R.html#exploring-the-legacy-of-redlining",
    "href": "tutorials/R/1-intro-R.html#exploring-the-legacy-of-redlining",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow in both R and python. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing.\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\nlibrary(dplyr)\ngdalcubes::gdalcubes_options(parallel = TRUE)"
  },
  {
    "objectID": "tutorials/R/1-intro-R.html#data-discovery",
    "href": "tutorials/R/1-intro-R.html#data-discovery",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "The first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox &lt;- c(xmin=-122.51, ymin=37.71, xmax=-122.36, ymax=37.81) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  ext_query(\"eo:cloud_cover\" &lt; 20) |&gt;\n  post_request()\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ncol &lt;- stac_image_collection(items$features, asset_names = c(\"B08\", \"B04\", \"SCL\"))\n\ncube &lt;- cube_view(srs =\"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  dx = 0.0001, dy = 0.0001, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\ndata &lt;-  raster_cube(col, cube, mask = mask)\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine tree cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi &lt;- data |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\n\nndvi_stars &lt;- st_as_stars(ndvi)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nmako &lt;- tm_scale_continuous(values = viridisLite::mako(30))\nfill &lt;- tm_scale_continuous(values = \"Greens\")\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako)"
  },
  {
    "objectID": "tutorials/R/1-intro-R.html#zonal-statistics",
    "href": "tutorials/R/1-intro-R.html#zonal-statistics",
    "title": "From NDVI to Environmental Justice",
    "section": "",
    "text": "In addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\nsf &lt;- \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\" |&gt;\n  st_read() |&gt;\n  st_make_valid() |&gt;\n  select(-label_coords)\n\n\npoly &lt;- ndvi |&gt; extract_geom(sf, FUN = mean, reduce_time = TRUE)\nsf$NDVI &lt;- poly$NDVI\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade, using tmap. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako) +\n  tm_shape(sf) + tm_polygons('NDVI', fill.scale = fill) +\n  tm_shape(sf) + tm_text(\"grade\", col=\"darkblue\", size=0.6) +\n  tm_legend_hide()\n\n\n\n\nAre historically redlined areas still less green?\n\nsf |&gt; \n  as_tibble() |&gt;\n  group_by(grade) |&gt; \n  summarise(ndvi = mean(NDVI), \n            sd = sd(NDVI)) |&gt;\n  knitr::kable()\n\n\n\n\ngrade\nndvi\nsd\n\n\n\n\nA\n0.3201204\n0.0611414\n\n\nB\n0.2138501\n0.0783221\n\n\nC\n0.1956334\n0.0564822\n\n\nD\n0.1949736\n0.0385805\n\n\nNA\n0.0962092\nNA"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "This project seeks to introduce cloud-native approaches to geospatial analysis in R & Python through the lens of environmental justice applications. This is not meant as a complete course in geospatial analysis – though we encourage interested readers to consider Geocomputation in R or Python as an excellent resource. We present opinionated recipes meant to empower users with the following design goals:\n\nOpen science: open source software, open data, open standards, and reproducibility are emphasized.\nRecipes are presented as reproducible computational notebooks (Quarto and Jupyter) set in narrative analysis.\nAccess to cloud-based compute is not required. The high speed networks of commercial cloud providers can accelerate network-bound operations, but can run from anywhere.\nAccess to a powerful laptop or desktop is not required either. Examples can run from a browser interface on free compute resources such as Codespaces.\nPortable software environments: a Docker container with precise software used to produce these examples. The container is build from the included Dockerfile using the included GitHub Action and is deployed by the Codespaces instance.\nAlignment between R and Python: recipes will try to show approaches in both languages and seek to utilize common interfaces (such as STAC and GDAL) that can be shared across these platforms.\n\nThis is a work in progress! Check back often, and feedback welcome! Test these modules, file an issue or pull request, launch into a Codespaces environment, or reach out on the discussion board.\nWhat is “cloud-native” anyway? We define cloud-native to mean simply that data is accessed over http range request methods, rather than downloading entire files. Code-based examples will develop why this is important and how it differs from renting cloud-based compute. The core philosophy is that what many users already know how to do locally translates pretty seamlessly here, and then a bit extra is required to coerce certain software to stay in ‘range request’ mode and not get greedy trying to download everything. Some authors define this concept somewhat differently, and certainly not all range requests give the same performance, nor are http range requests best in all cases.\n\n\nThanks to input, suggestions, feedback and ideas from so many amazing folks in the OpenScapes community and input and financial support from the NASA TOPS community."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "",
    "section": "",
    "text": "Thanks to input, suggestions, feedback and ideas from so many amazing folks in the OpenScapes community and input and financial support from the NASA TOPS community."
  },
  {
    "objectID": "tutorials/R/2-earthdata.html",
    "href": "tutorials/R/2-earthdata.html",
    "title": "Search via STAC",
    "section": "",
    "text": "This tutorial demonstrates how begin to generalize the pattern introduced in the introductory example to work with other data sources, in particular, with NASA EarthData.\nNASA recently announced completion of the transfer of some 59 petabytes of data to the Amazon cloud – a core component of NASA’s Transformation to Open Science (TOPS) mission. Researchers are frequently told that to take advantage of such “cloud data” they will need to pay (or find a grant or other program to pay) for cloud computing resources. This approach is sometimes describes as “send the compute to the data”. While this narrative is no doubt beneficial to Amazon Inc, it exacerbates inequity and is often misleading. The purpose of having data in a cloud storage platform is not just to make it faster to access that data on rented cloud computing platforms. The high bandwith and high disk speeds provided by these systems can be just as powerful when you provide your own compute. Consistent with NASA’s vision, this means that high-performance access is free\n\nNASA Earth science data have been freely openly and available to all users since EOSDIS became operational in 1994. Under NASA’s full and open data policy, all NASA mission data (along with the algorithms, metadata, and documentation associated with these data) must be freely available to the public. This means that anyone, anywhere in the world, can access the more than 59 PB of NASA Earth science data without restriction\n\nAll we need is software that can treat the cloud storage as if it were local storage: a virtual filesystem. The ability to do this – the HTTP range request standard – has been around for over two decades and is widely implemented in open source software. Unfortunately, many users and workflows are stuck in an old model that assumes individual files must always be downloaded first.\nTo make this work with NASA EarthData however, we have one additional challenge involving the problem of authentication. NASA offers several different mechanisms, including (1) netrc authentication, (2) token-based authentication, and (3) S3 credentials, but only the first of these works equally well from locations both inside and outside of AWS-based compute, so there really is very little reason to learn the other two.\nThe earthdatalogin package in R or the earthaccess package in Python handle the authentication. The R package sets up authentication behind the scenes using environmental variables.\n\nearthdatalogin::edl_netrc()\n\n(A default login is supplied though users are encouraged to register for their own individual accounts.) Once this is in place, EarthData’s protected URLs can be used like any other. For instance, after authenticating, we can read this NASA harmonized LandSat-Sentinel2 tif whether we are running locally:\n\nterra::rast(\"https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T56JKT.2023246T235950.v2.0/HLS.L30.T56JKT.2023246T235950.v2.0.SAA.tif\",\n     vsi=TRUE)\n\nclass       : SpatRaster \ndimensions  : 3660, 3660, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 199980, 309780, 7190200, 7300000  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 56N (EPSG:32656) \nsource      : HLS.L30.T56JKT.2023246T235950.v2.0.SAA.tif \nname        : HLS.L30.T56JKT.2023246T235950.v2.0.SAA \n\n\nAn important aspect about this approach is that it does not require any custom wrappers or specialized functions to access data. Most R packages that work with spatial data, including terra, sf, stars and others, do so through the use of GDAL for parsing spatial data formats. This means that they all support the GDAL Virtual Filesystem for cloud-native reads out of the box. earthdatalogin() takes advantage of this by setting authentication credentials as GDAL environmental variable configuration, allowing these existing to seamlessly read NASA data. No need to learn any additional access functions.\n\n\n\nlibrary(earthdatalogin)\nlibrary(rstac)\nlibrary(gdalcubes)\n\nearthdatalogin also includes optional configuration settings for GDAL which can improve performance of cloud-based data access. Set the GDAL environmental variables using gdal_cloud_config(). An additional helper function exposes the usual GDAL environmental variable to the gdalcubes R package.\n\nedl_netrc()           # Authenticate\ngdal_cloud_config()   # Optimize GDAL for cloud\nwith_gdalcubes()      # Export settings to gdalcubes package\n\ngdalcubes_options(parallel = TRUE)\n\nNASA provides their own search system. NASA also provides a STAC-based search (see below), which allows us to use the standard syntax we have already seen. However, NASA’s STAC API is significantly slower and more prone to server errors than the STAC APIs provided by Element84, Microsoft Planetary Computer, and others.\nHere, we use NASA’s own search protocol, which is less general, but gives us a chance to illustrate the use of gdalcubes using arbitrary URL lists when no STAC catalog is available. We search a handful of dates for illustrative purposes, but this approach can easily scale to larger lists without needing additional RAM or disk space.\n\nstart &lt;- \"2020-01-01\"\nend   &lt;- \"2020-01-03\" \nurls &lt;- edl_search(short_name = \"MUR-JPL-L4-GLOB-v4.1\",\n                   temporal = c(start, end))\n\nThese netcdf files lack appropriate metadata (projection, extent) that GDAL expects. We can provide this manually using the GDAL VRT mechanism:\n\nvrt &lt;- function(url) {\n  prefix &lt;-  \"vrt://NETCDF:/vsicurl/\"\n  suffix &lt;- \":analysed_sst?a_srs=OGC:CRS84&a_ullr=-180,90,180,-90\"\n  paste0(prefix, url, suffix)\n}\n\n# date associated with each file\nurl_dates &lt;- as.Date(gsub(\".*(\\\\d{8})\\\\d{6}.*\", \"\\\\1\", urls), format=\"%Y%m%d\")\n\nBecause each file in this list of URLs has the same spatial extent, resolution, and projection, we can now manually construct our space-time data cube from these netcdf slices:\n\ndata_gd &lt;- gdalcubes::stack_cube(vrt(urls), datetime_values = url_dates)\n\nWe use gdalcubes to crop each file\n\nextent = list(left=-93, right=-76, bottom=41, top=49,\n              t0=start, t1=end)\n\nbench::bench_time({\n  data_gd |&gt; \n    gdalcubes::crop(extent) |&gt; \n    aggregate_time(dt=\"P3D\", method=\"mean\") |&gt; \n    plot(col = viridisLite::viridis(10))\n})\n\n\n\n\nprocess    real \n  2.29s  27.34s \n\n\n\n\n\nThe STAC Catalog search system we illustrated in the introductory example is a widely used standard. This means that we can use the same packages and same code we have already learned to access entirely different geospatial data products prepared by an entirely different provider. In this example, we will illustrate searching NASA’s EarthData catalog using the STAC interface. We can also browse the collection of NASA Earthdata STAC Catalogs in a web browser. (NOTE: Unfortunately, at this time, NASA’s implementation of the STAC standard is incomplete. The web browser only shows the first 10 entries under any heading, despite the NASA STAC API actually having the complete records. Also, NASA’s STAC service is considerably slower than those provided by Element84 or Microsoft Planetary Computer.)\n\nitems &lt;- stac(\"https://cmr.earthdata.nasa.gov/stac/POCLOUD\") |&gt; \n  stac_search(collections = \"MUR-JPL-L4-GLOB-v4.1\",\n              datetime = paste(start,end, sep = \"/\")) |&gt;\n  post_request() |&gt;\n  items_fetch()\n\nitems\n\n###STACItemCollection\n- matched feature(s): 3\n- features (3 item(s) / 0 not fetched):\n  - 20200101090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n  - 20200102090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n  - 20200103090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n- assets: data, metadata, opendap\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type"
  },
  {
    "objectID": "tutorials/R/2-earthdata.html#working-with-stac-gdalcubes",
    "href": "tutorials/R/2-earthdata.html#working-with-stac-gdalcubes",
    "title": "Search via STAC",
    "section": "",
    "text": "library(earthdatalogin)\nlibrary(rstac)\nlibrary(gdalcubes)\n\nearthdatalogin also includes optional configuration settings for GDAL which can improve performance of cloud-based data access. Set the GDAL environmental variables using gdal_cloud_config(). An additional helper function exposes the usual GDAL environmental variable to the gdalcubes R package.\n\nedl_netrc()           # Authenticate\ngdal_cloud_config()   # Optimize GDAL for cloud\nwith_gdalcubes()      # Export settings to gdalcubes package\n\ngdalcubes_options(parallel = TRUE)\n\nNASA provides their own search system. NASA also provides a STAC-based search (see below), which allows us to use the standard syntax we have already seen. However, NASA’s STAC API is significantly slower and more prone to server errors than the STAC APIs provided by Element84, Microsoft Planetary Computer, and others.\nHere, we use NASA’s own search protocol, which is less general, but gives us a chance to illustrate the use of gdalcubes using arbitrary URL lists when no STAC catalog is available. We search a handful of dates for illustrative purposes, but this approach can easily scale to larger lists without needing additional RAM or disk space.\n\nstart &lt;- \"2020-01-01\"\nend   &lt;- \"2020-01-03\" \nurls &lt;- edl_search(short_name = \"MUR-JPL-L4-GLOB-v4.1\",\n                   temporal = c(start, end))\n\nThese netcdf files lack appropriate metadata (projection, extent) that GDAL expects. We can provide this manually using the GDAL VRT mechanism:\n\nvrt &lt;- function(url) {\n  prefix &lt;-  \"vrt://NETCDF:/vsicurl/\"\n  suffix &lt;- \":analysed_sst?a_srs=OGC:CRS84&a_ullr=-180,90,180,-90\"\n  paste0(prefix, url, suffix)\n}\n\n# date associated with each file\nurl_dates &lt;- as.Date(gsub(\".*(\\\\d{8})\\\\d{6}.*\", \"\\\\1\", urls), format=\"%Y%m%d\")\n\nBecause each file in this list of URLs has the same spatial extent, resolution, and projection, we can now manually construct our space-time data cube from these netcdf slices:\n\ndata_gd &lt;- gdalcubes::stack_cube(vrt(urls), datetime_values = url_dates)\n\nWe use gdalcubes to crop each file\n\nextent = list(left=-93, right=-76, bottom=41, top=49,\n              t0=start, t1=end)\n\nbench::bench_time({\n  data_gd |&gt; \n    gdalcubes::crop(extent) |&gt; \n    aggregate_time(dt=\"P3D\", method=\"mean\") |&gt; \n    plot(col = viridisLite::viridis(10))\n})\n\n\n\n\nprocess    real \n  2.29s  27.34s"
  },
  {
    "objectID": "tutorials/python/1-intro-python.html",
    "href": "tutorials/python/1-intro-python.html",
    "title": "Exploring the Legacy of Redlining",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing.\n\nfrom pystac_client import Client\nimport odc.stac\nimport pystac_client\nimport rioxarray\nimport geopandas as gpd\nfrom rasterstats import zonal_stats \n\n# suppress warning messages\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nThe first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox = [-122.51, 37.71, -122.36, 37.81]\nitems = (\n  Client.\n  open(\"https://earth-search.aws.element84.com/v1\").\n  search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}).\n  item_collection()\n)\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:32610\",\n    bands=[\"nir08\", \"red\"],\n    bbox=box\n)\n# For some reason, requesting reprojection in EPSG:4326 gives only empty values\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine vegetation cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi = (((data.nir08 - data.red) / (data.red + data.nir08)).\n        resample(time=\"MS\").\n        median(\"time\", keep_attrs=True).\n        compute()\n)\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=4)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fa18b685fc0&gt;\n\n\n\n\n\n\n\n\n\nWe examine the present-day impact of historic “red-lining” of US cities during the Great Depression using data from the Mapping Inequality project. All though this racist practice was banned by federal law under the Fair Housing Act of 1968, the systemic scars of that practice are still so deeply etched on our landscape that the remain visible from space – “red-lined” areas (graded “D” under the racist HOLC scheme) show systematically lower greenness than predominately-white neighborhoods (Grade “A”). Trees provide many benefits, from mitigating urban heat to biodiversity, real-estate value, to health.\n\n\nIn addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\n# make sure we are in the same projection.\n# zonal_stats method understands paths but not xarray\n(   ndvi.\n    rio.reproject(\"EPSG:4326\").\n    rio.to_raster(raster_path=\"ndvi.tif\", driver=\"COG\")   \n)\n\nsf_url = \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\"\nmean_ndvi = zonal_stats(sf_url, \"ndvi.tif\", stats=\"mean\")\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\nsf = gpd.read_file(sf_url)\nsf[\"ndvi\"] = [x[\"mean\"] for x in mean_ndvi]\nsf.plot(column=\"ndvi\", legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nAre historically redlined areas still less green?\n\nimport geopolars as gpl\nimport polars as pl\n\n(gpl.\n  from_geopandas(sf).\n  group_by(\"grade\").\n  agg(pl.col(\"ndvi\").mean()).\n  sort(\"grade\")\n)\n\n\nshape: (5, 2)\n\n\n\ngrade\nndvi\n\n\nstr\nf64\n\n\n\n\nnull\n0.159305\n\n\n\"A\"\n0.337976\n\n\n\"B\"\n0.247661\n\n\n\"C\"\n0.236407\n\n\n\"D\"\n0.232125"
  },
  {
    "objectID": "tutorials/python/1-intro-python.html#data-discovery",
    "href": "tutorials/python/1-intro-python.html#data-discovery",
    "title": "Exploring the Legacy of Redlining",
    "section": "",
    "text": "The first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nbox = [-122.51, 37.71, -122.36, 37.81]\nitems = (\n  Client.\n  open(\"https://earth-search.aws.element84.com/v1\").\n  search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}).\n  item_collection()\n)\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:32610\",\n    bands=[\"nir08\", \"red\"],\n    bbox=box\n)\n# For some reason, requesting reprojection in EPSG:4326 gives only empty values\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine vegetation cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nndvi = (((data.nir08 - data.red) / (data.red + data.nir08)).\n        resample(time=\"MS\").\n        median(\"time\", keep_attrs=True).\n        compute()\n)\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=4)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fa18b685fc0&gt;"
  },
  {
    "objectID": "tutorials/python/1-intro-python.html#zonal-statistics",
    "href": "tutorials/python/1-intro-python.html#zonal-statistics",
    "title": "Exploring the Legacy of Redlining",
    "section": "",
    "text": "In addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\n# make sure we are in the same projection.\n# zonal_stats method understands paths but not xarray\n(   ndvi.\n    rio.reproject(\"EPSG:4326\").\n    rio.to_raster(raster_path=\"ndvi.tif\", driver=\"COG\")   \n)\n\nsf_url = \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\"\nmean_ndvi = zonal_stats(sf_url, \"ndvi.tif\", stats=\"mean\")\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\nsf = gpd.read_file(sf_url)\nsf[\"ndvi\"] = [x[\"mean\"] for x in mean_ndvi]\nsf.plot(column=\"ndvi\", legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nAre historically redlined areas still less green?\n\nimport geopolars as gpl\nimport polars as pl\n\n(gpl.\n  from_geopandas(sf).\n  group_by(\"grade\").\n  agg(pl.col(\"ndvi\").mean()).\n  sort(\"grade\")\n)\n\n\nshape: (5, 2)\n\n\n\ngrade\nndvi\n\n\nstr\nf64\n\n\n\n\nnull\n0.159305\n\n\n\"A\"\n0.337976\n\n\n\"B\"\n0.247661\n\n\n\"C\"\n0.236407\n\n\n\"D\"\n0.232125"
  }
]