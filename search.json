[
  {
    "objectID": "contents/python-ex.html",
    "href": "contents/python-ex.html",
    "title": "",
    "section": "",
    "text": "# Examining Environmental Justice through Open Source, Cloud-Native Tools: R\nThis Jupyter (ipynb) notebook provides a brief introduction to a cloud-native workflow. A more complete version of this example can be found in the intro.qmd Quarto notebook. Here, we merely include an ipynb version to demonstrate how Jupyter notebooks may be included in quarto.\nNote: in Codespaces editor, choose “Select Kernel” -&gt; “Jupyter Kernels” to get started.\nDeveloper note: At this time, Gitpod VSCode editor will not detect the default python environment (VIRTUAL_ENV variable, pointing to /opt/venv/bin/python), or any other kernelspec added with ipykernel.\n\nfrom pystac_client import Client\nimport odc.stac\nimport pystac_client\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nbox = [-122.51006, 37.70801, -122.36268, 37.80668]\nclient = Client.open(\"https://earth-search.aws.element84.com/v1\")\nsearch = client.search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}\n)\n\n\nitems = search.get_all_items()\n# items[0]  # peak at an item\n\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:32610\",\n    bands=[\"nir08\", \"red\"],\n    resolution=30,\n    bbox=box\n)\n\n\nred = data.red\nnir = data.nir08\n\n# summarize over time. \n# quite probably better to use resampling strategy in odc.stac.load though.\nimport dask.diagnostics\nwith dask.diagnostics.ProgressBar():\n    ndvi = ( ((nir - red) / (red + nir)).\n            resample(time=\"MS\").\n            median(\"time\", keep_attrs=True).\n            compute()\n    )\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=4)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fec26fd96c0&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NASA TOPS-T: Cloud Native Geospatial in R & Python",
    "section": "",
    "text": "This project seeks to introduce cloud-native approaches to geospatial analysis in R & Python through the lens of environmental justice applications. This is not meant as a complete course in geospatial analysis – though we encourage interested readers to consider Geocomputation in R or Python as an excellent resource. We present opinionated recipes meant to empower users with the following design goals:\nThis is a work in progress! Check back often, and feedback welcome! Test these modules, file an issue or pull request, launch into a Codespaces environment, or reach out on the discussion board.\nWhat is “cloud-native” anyway? We define cloud-native to mean simply that data is accessed over http range request methods, rather than downloading entire files. Code-based examples will develop why this is important and how it differs from renting cloud-based compute. The core philosophy is that what many users already know how to do locally translates pretty seamlessly here, and then a bit extra is required to coerce certain software to stay in ‘range request’ mode and not get greedy trying to download everything. Some authors define this concept somewhat differently, and certainly not all range requests give the same performance, nor are http range requests best in all cases."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "NASA TOPS-T: Cloud Native Geospatial in R & Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to input, suggestions, feedback and ideas from so many amazing folks in the OpenScapes community and input and financial support from the NASA TOPS community."
  },
  {
    "objectID": "contents/earthdata.html",
    "href": "contents/earthdata.html",
    "title": "NASA EarthData",
    "section": "",
    "text": "The NASA EarthData program provides access to an extensive collection of spatial data products from each of it’s 12 Distributed Active Archive Centers (‘DAACs’) on the high-performance S3 storage system of Amazon Web Services (AWS).\n…"
  },
  {
    "objectID": "contents/computing-environment.html",
    "href": "contents/computing-environment.html",
    "title": "Computing Environments",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud."
  },
  {
    "objectID": "contents/computing-environment.html#portable-reproducibility",
    "href": "contents/computing-environment.html#portable-reproducibility",
    "title": "Computing Environments",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud."
  },
  {
    "objectID": "contents/computing-environment.html#notebooks-on-github",
    "href": "contents/computing-environment.html#notebooks-on-github",
    "title": "Computing Environments",
    "section": "Notebooks on GitHub",
    "text": "Notebooks on GitHub\nEach of the recipes on this site correspond to a Quarto or Jupyter notebook in a GitHub repository (see contents/ directory or footer links). Such notebooks form the basis of technical documentation and publishing to a wide array of formats. A _quarto.yml configuration file in the repository root determines the website layout. Notebooks can be run interactively in any appropriate environment (RStudio, JupyterLab, VSCode, etc, see below for free online platforms). Type quarto render from the bash command line or use quarto::quarto_preview() from the R console to preview the entire site."
  },
  {
    "objectID": "contents/computing-environment.html#a-portable-compute-environment",
    "href": "contents/computing-environment.html#a-portable-compute-environment",
    "title": "Computing Environments",
    "section": "A Portable compute environment",
    "text": "A Portable compute environment"
  },
  {
    "objectID": "contents/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "href": "contents/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "title": "Computing Environments",
    "section": "On the cloud: Gitpod or Codespaces",
    "text": "On the cloud: Gitpod or Codespaces\n \nBoth GitHub Codespaces and Gitpod provide a fast and simple way to enter into integrated development environments such as VSCode, RStudio, or JupyterLab on free, cloud-based virtual machines. Codespaces has a free tier of 60 hours/month, Gitpod of 50 hours a month, both offer paid plans for additional use and larger compute instances. Small codespace instances are also free to instructors\nBy clicking on one of the buttons in GitHub, users will be placed into a free cloud-based virtual machine running a VSCode editor in their web browser. The first setup can take a few minutes to complete.\nAdditionally, this also provides access to an RStudio environment on an embedded port for users who prefer that editor to VSCode. Once the Codespace has fully completed loading, it will include a link in a second welcome message in the Terminal to access RStudio like so:\n\nThe RStudio link can also always be accessed from the Ports tab under the port labeled “Rstudio” (8787). (Gitpod will show a pop-up message to open this port instead.)\nBoth Codespaces and Gitpod can be configured with custom compute environments by supplying a docker image. Both the VSCode and RStudio editors run in the same underlying custom Docker container. This repository includes a Dockerfile defining this compute environment which includes specific versions of R and python packages, the latest releases of the OSGeo C libraries GDAL, PROJ, and GEOS that power many spatial operations in both languages. These elements are pre-compiled in a Docker container based on the latest Ubuntu LTS release (22.04 at the time of writing), which itself is build according to the Dockerfile found in this repository using a GitHub Action. The devcontainer.json configuration will also set up relevant VSCode extensions for both both Jupyter and Quarto notebooks, with each supporting both R and Python."
  },
  {
    "objectID": "contents/computing-environment.html#locally-vscode",
    "href": "contents/computing-environment.html#locally-vscode",
    "title": "Computing Environments",
    "section": "Locally: VSCode",
    "text": "Locally: VSCode\nOpen this repository in a local Visual Studio Code editor on a Mac, Linux, or Windows laptop and you will probably be prompted “Do you want to open this in project in a Dev Container?” If you agree, VSCode will attempt to use a local Docker installation to pull a container with much of the required software already installed. This uses the same Docker container and enables all the same extensions in VSCode, including RStudio server on the embedded port.\nOf course, users can open this project in a local VSCode or any other favorite editor without opening in the devcontainer. The user assumes responsibility to install necessary software, i.e. the packages listed in requirements.txt or install.R. Note that doing so does not ensure that the same version of system libraries like GDAL, PROJ, or GEOS will necessarily be used. For most operations this should not matter, but users on older versions of GDAL may encounter worse performance or other difficulties."
  },
  {
    "objectID": "contents/computing-environment.html#anywhere-docker",
    "href": "contents/computing-environment.html#anywhere-docker",
    "title": "Computing Environments",
    "section": "Anywhere: Docker",
    "text": "Anywhere: Docker\nWe can sidesteps elements specific to the VSCode editor defined in the devcontainer.json configuration while still leveraging the same system libraries and pre-built packages. For example, a user could also choose to run (or extend) the underlying docker container independently, e.g.\ndocker run --rm -ti ghcr.io/boettiger-lab/nasa-tops:latest bash\nwhich opens a bash terminal inside the container. This approach is also compatible with most HPC setups using singularity instead of docker.\nSome users may not be familiar with editing and running code entirely from a bash shell, so the container also includes RStudio server and thus can be run to launch RStudio in an open port instead,\ndocker run -d -p 8787:8787 --user root -e DISABLE_AUTH=true \\\n  ghcr.io/boettiger-lab/nasa-tops:latest\nand visit http://localhost:8787 to connect."
  },
  {
    "objectID": "contents/intro.html",
    "href": "contents/intro.html",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow in both R and python. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing."
  },
  {
    "objectID": "contents/intro.html#data-discovery",
    "href": "contents/intro.html#data-discovery",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools",
    "section": "Data discovery",
    "text": "Data discovery\n\nThe first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nRPython\n\n\n\nbox &lt;- c(xmin=-122.51, ymin=37.71, xmax=-122.36, ymax=37.81) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  ext_query(\"eo:cloud_cover\" &lt; 20) |&gt;\n  post_request()\n\n\n\n\nbox = [-122.51, 37.71, -122.36, 37.81]\nitems = (\n  Client.\n  open(\"https://earth-search.aws.element84.com/v1\").\n  search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}).\n  item_collection()\n)\n\n\n\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\nRPython\n\n\n\ncol &lt;- stac_image_collection(items$features, asset_names = c(\"B08\", \"B04\", \"SCL\"))\n\ncube &lt;- cube_view(srs =\"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  dx = 0.0001, dy = 0.0001, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\ndata &lt;-  raster_cube(col, cube, mask = mask)\n\n\n\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:4326\",\n    bands=[\"nir08\", \"red\"],\n    resolution=0.0001,\n    bbox=box\n)\n\n\n\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine tree cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nRPython\n\n\n\nndvi &lt;- data |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\n\nndvi_stars &lt;- st_as_stars(ndvi)\n\n\n\n\nred = data.red\nnir = data.nir08\n\nndvi = (((nir - red) / (red + nir)).\n        resample(time=\"MS\").\n        median(\"time\", keep_attrs=True).\n        compute()\n)\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\n\n\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nRPython\n\n\n\nmako &lt;- tm_scale_continuous(values = viridisLite::mako(30))\nfill &lt;- tm_scale_continuous(values = \"Greens\")\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako)\n\n\n\n\n\n\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=5)\n\n&lt;xarray.plot.facetgrid.FacetGrid object at 0x7fa1186efe20&gt;"
  },
  {
    "objectID": "contents/intro.html#zonal-statistics",
    "href": "contents/intro.html#zonal-statistics",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools",
    "section": "Zonal statistics",
    "text": "Zonal statistics\nIn addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\nRPython\n\n\n\nsf &lt;- st_read(\"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/CASanFrancisco1937.geojson\") |&gt;\n  st_make_valid()\npoly &lt;- ndvi |&gt; extract_geom(sf, FUN = mean, reduce_time = TRUE)\nsf$NDVI &lt;- poly$NDVI\n\n\n\n\nndvi.rio.to_raster(raster_path=\"ndvi.tif\", driver=\"COG\")\nsf_url = \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/CASanFrancisco1937.geojson\"\nmean_ndvi = zonal_stats(sf_url, \"ndvi.tif\", stats=\"mean\")\n\n/home/cboettig/.local/lib/python3.10/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\n\n\n\n\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade, using tmap. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\nRPython\n\n\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako) +\n  tm_shape(sf) + tm_polygons('NDVI', fill.scale = fill) +\n  tm_shape(sf) + tm_text(\"holc_grade\", col=\"darkblue\", size=0.6) +\n  tm_legend_hide()\n\n\n\n\n\n\n\nsf = gpd.read_file(sf_url)\nsf[\"ndvi\"] = [x[\"mean\"] for x in mean_ndvi ]\nsf.plot(column=\"ndvi\", legend=True)\n\n\n\n\n\n\n\nAre historically redlined areas still less green?\n\nRPython\n\n\n\nsf |&gt; \n  as_tibble() |&gt;\n  group_by(holc_grade) |&gt; \n  summarise(ndvi = mean(NDVI), \n            sd = sd(NDVI)) |&gt;\n  knitr::kable()\n\n\n\n\nholc_grade\nndvi\nsd\n\n\n\n\nA\n0.3201691\n0.0611824\n\n\nB\n0.2138027\n0.0783241\n\n\nC\n0.1955999\n0.0564923\n\n\nD\n0.1949583\n0.0385485\n\n\n\n\n\n\n\n\nimport geopolars as gpl\nimport polars as pl\n\n(gpl.\n  from_geopandas(sf).\n  group_by(\"holc_grade\").\n  agg(pl.col(\"ndvi\").mean()).\n  sort(\"holc_grade\")\n)\n\n\nshape: (4, 2)\n\n\n\nholc_grade\nndvi\n\n\nstr\nf64\n\n\n\n\n\"A\"\n0.338817\n\n\n\"B\"\n0.247294\n\n\n\"C\"\n0.231133\n\n\n\"D\"\n0.225662"
  },
  {
    "objectID": "contents/R-ex.html",
    "href": "contents/R-ex.html",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools: R",
    "section": "",
    "text": "This Jupyter (ipynb) notebook provides a brief introduction to a cloud-native workflow. A more complete version of this example can be found in the intro.qmd Quarto notebook. Here, we merely include an ipynb version to demonstrate how Jupyter notebooks may be included in quarto and, in this case, execute in an R kernel.\nNote: in Codespaces editor, choose “Select Kernel” -&gt; “Jupyter Kernels” -&gt; “R” to get started.\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\ngdalcubes::gdalcubes_options(parallel = TRUE)\n\n\nbox &lt;- c(xmin=-122.51006, ymin=37.70801, xmax=-122.36268, ymax=37.80668) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  post_request()\n\n\n\ncol &lt;- stac_image_collection(items$features,\n                             asset_names = c(\"B04\", \"B08\", \"SCL\"),\n                             property_filter = \\(x){\n                               x[[\"eo:cloud_cover\"]] &lt; 20\n                             })\n\ncube &lt;- cube_view(srs = \"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  nx = 1200, ny = 1200, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\n\n\nndvi &lt;- raster_cube(col, cube, mask = mask) |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\nplot(ndvi)"
  }
]