[
  {
    "objectID": "contents/xarray-benchmark.html",
    "href": "contents/xarray-benchmark.html",
    "title": "Cloud-native EarthData Access",
    "section": "",
    "text": "We show how to run existing NASA data workflows on the cloud, in parallel, without a cloud service account using your laptop.\nNASA recently announced completion of the transfer of some 59 petabytes of data to the Amazon cloud – a core component of NASA’s Transformation to Open Science (TOPS) mission. Researchers are frequently told that to take advantage of such “cloud data” they will need to pay (or find a grant or other program to pay) for cloud computing resources. This approach is sometimes describes as “send the compute to the data”. While this narrative is no doubt beneficial to Amazon Inc, it exacerbates inequity and is often misleading. The purpose of having data in a cloud storage platform is not just to make it faster to access that data on rented cloud computing platforms. The high bandwith and high disk speeds provided by these systems can be just as powerful when you provide your own compute. Consistent with NASA’s vision, this means that high-performance access is free\nAll we need is software that can treat the cloud storage as if it were local storage: a virtual filesystem. The ability to do this – the HTTP range request standard – has been around for over two decades and is widely implemented in open source software. Unfortunately, many users and workflows are stuck in an old model that assumes individual files must always be downloaded first. We even see this in workflows designed to run on cloud platforms. A recent blog post by commercial platform Coiled provides a typical illustration of this. Typical of most workflows, the example relies on only a small subset of the data found in each file (specific spatial region and specific variables), but uses an approach that downloads all 500+ GB of files anyway. A faster approach will only download the range of data that we actually use. The Coiled blog post shows a full download-based workflow that takes 6.4hrs to complete locally, and then argues that by adding some extra code and paying Amazon for compute time, the same workflow can run much faster. Here we will consider the same workflow again, but this time make it much faster just by removing the download loop and running it from a local machine for free. We also compare a second way that dispatches via a different virtual filesystem approach and is even faster.\nimport earthaccess\nimport rioxarray\nimport rasterio\nimport xarray as xr\nimport numpy as np\nresults = earthaccess.search_data(\n    short_name=\"MUR-JPL-L4-GLOB-v4.1\",\n    temporal=(\"2020-01-01\", \"2021-12-31\"),\n    #temporal=(\"2019-01-01\", \"2019-01-31\"),\n)\n\nGranules found: 729"
  },
  {
    "objectID": "contents/xarray-benchmark.html#using-fsspec-virtual-filesystem",
    "href": "contents/xarray-benchmark.html#using-fsspec-virtual-filesystem",
    "title": "Cloud-native EarthData Access",
    "section": "Using fsspec virtual filesystem",
    "text": "Using fsspec virtual filesystem\nRather than tediously downloading the entirety of each file and then manually looping over each one to open, read, and concatinate as shown in the Coiled Demo, we can simply open the whole set in one go. This lazy read method allows us to then range-request only the subset of data we need from each file, thanks to earthaccess using a cloud-native reads over http via the python fsspec package. We can then issue the usual xarray operations to process and plot the data, treating the remote source as if it were already sitting on our local disk. This approach is quite fast, works from any machine, and does not require spending money on AWS. (note that fsspec package, which is doing the core magic of allowing us to treat the remote filesystem as if it were a local filesystem, is not explicitly visible in this workflow, but earthaccess has taken care of it for us).\nNote this code is also simpler and more concise than the implementation shown in download-based workflow. Setting up the fsspec connections takes about 29 minutes. because the evaluation is lazy, most of the computation only occurs at the last step, when we create the plot, which takes about 46 minutes.\n\n%%time\nfiles = earthaccess.open(results)\nds = xr.open_mfdataset(files,\n                       decode_times=False, \n                       data_vars=['analysed_sst', 'sea_ice_fraction'], \n                       concat_dim=\"time\", \n                       combine=\"nested\",\n                       parallel=True)\n\nOpening 729 granules, approx size: 505.34 GB\nCPU times: user 2min 12s, sys: 35.9 s, total: 2min 48s\nWall time: 29min 35s\n\n\n\n\n\n\n\n\n\n\n\n\n%%time\ndds = ds.sel(lon=slice(-93, -76), lat=slice(41, 49))\ncond = (dds.sea_ice_fraction &lt; 0.15) | np.isnan(dds.sea_ice_fraction)\nresult = dds.analysed_sst.where(cond)\n\nCPU times: user 21.3 ms, sys: 0 ns, total: 21.3 ms\nWall time: 19.4 ms\n\n\n\n%%time\nresult.std(\"time\").plot(figsize=(14, 6), x=\"lon\", y=\"lat\")\n\n/home/jovyan/.virtualenvs/spatial/lib/python3.10/site-packages/dask/array/numpy_compat.py:51: RuntimeWarning: invalid value encountered in divide\n  x = np.divide(x1, x2, out)\n\n\nCPU times: user 3min 51s, sys: 1min 28s, total: 5min 19s\nWall time: 46min 19s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f4dfd91b370&gt;"
  },
  {
    "objectID": "contents/xarray-benchmark.html#using-gdal-virtual-filesystem",
    "href": "contents/xarray-benchmark.html#using-gdal-virtual-filesystem",
    "title": "Cloud-native EarthData Access",
    "section": "Using GDAL Virtual Filesystem",
    "text": "Using GDAL Virtual Filesystem\nA different virtual filesystem approach is available through GDAL. While fsspec tries to provide a generic POSIX-like interface to remote files, the GDAL VSI is specifically optimized for spatial data and often considerably faster. The rioxarray package provides a drop-in engine to xarray’s open_mfdataset that uses GDAL. (Aside – at least some of the other netcdf engines supported by xarray should also be able to natively perform range requests over URLs to data without needing the fsspec layer added by earthaccess, and may have better performance. This case is not illustrated in this notebook). Here we’ll use the GDAL VSI.\nBecause the NASA EarthData are behind a security layer, using the URLs directly instead of earthaccess with fsspec requires a little extra handling of authentication process to make GDAL aware of the NETRC and cookie files it needs. We’ll also set some of the optional but recommended options for GDAL when using the virtual filesystem. Unfortunately this makes our code look a bit verbose – ideally packages like rioxarray would take care of these things.\nNote the GDAL is about 3x faster at setting up the virtual filesystem, and a little faster in the xarray/dask dispatch to compute the plot. (When this approach is combined with metadata from a STAC catalog, it does not need to read individual file metadata and the first step can become almost instant).\n\n%%time\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom pathlib import Path\ncookies = os.path.expanduser(\"~/.urs_cookies\")\nPath(cookies).touch()\n\n## pull out the URLs\ndata_links = [granule.data_links(access=\"external\") for granule in results]\nurl_links = [f'{link[0]}' for link in data_links]\n\n\n# and here we go\nwith rasterio.Env(GDAL_INGESTED_BYTES_AT_OPEN=\"32000\",\n                  GDAL_HTTP_MULTIPLEX=\"YES\",\n                  GDAL_HTTP_MERGE_CONSECUTIVE_RANGES=\"YES\",\n                  GDAL_HTTP_VERSION=\"2\",\n                  GDAL_NUM_THREADS=\"ALL_CPUS\",\n                  GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n                  GDAL_HTTP_COOKIEFILE=cookies, \n                  GDAL_HTTP_COOKIEJAR=cookies, \n                  GDAL_HTTP_NETRC=True):\n    ds1 = xr.open_mfdataset(url_links, \n                           engine = \"rasterio\", \n                           concat_dim=\"time\", \n                           combine=\"nested\",\n                           )\n\nCPU times: user 47.7 s, sys: 3.78 s, total: 51.4 s\nWall time: 11min 48s\n\n\n\n%%time\ndds = ds1.sel(x=slice(18000-9300, 18000-7600), y = slice(9000+4100,9000+4900))\ndds.analysed_sst.std(\"time\").plot(figsize=(14, 6), x=\"x\", y=\"y\")\n\nCPU times: user 2min 13s, sys: 3min 13s, total: 5min 27s\nWall time: 32min 55s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f4cf8396b30&gt;"
  },
  {
    "objectID": "contents/xarray-benchmark.html#comparisons",
    "href": "contents/xarray-benchmark.html#comparisons",
    "title": "Cloud-native EarthData Access",
    "section": "Comparisons",
    "text": "Comparisons\nThe GDAL VSI is already widely used under the hood by python packages working with cloud-optimized geotiff (COG) files (e.g. via odc.stac, which like the above approach also produces dask-backed xarrays), and also widely used by most other languages (e.g. R) for working with any spatial data. To GDAL, netcdf and other so-called “n-dimensional array” formats like h5, zarr are just a handful of the 160-odd formats of “raster” data it supports, along with formats like COG and GeoTIFF files. It can be particularly powerful in more complicated workflows which require spatially-aware operations such as reprojection and aggregation. The GDAL VSI can sometimes be considerably faster than fsspec, expecially when configured for cloud-native access. The nusiance of these environmental variables aside, it can also be considerably easier to use and to generalize patterns across data formats (netcdf, zarr, COG), and across languages (R, C++, javascript, julia etc), since GDAL understands [all these formats] and is used in all of these languages, as well as in platforms such as Google Earth Engine and QGIS. This makes it a natural bridge between languages. This broad use over decades has made GDAL very powerful, and it continues to improve rapidly with frequent releases.\nFor some reason, the xarray community seems to prefer to access ncdf without GDAL, whether by relying on downloading complete files, using fsspec, or other dedicated libraries (zarr). There are possibly many reasons for this. One is a divide between the the “Geospatial Information Systems” community, that thinks of file serializations as “rasters” or “vectors”, and the “modeler” community, which thinks of data as “n-dimensional arrays”. Both have their weaknesses and the lines are frequently blurred, but one obvious manifestation is in how each one writes their netcdf files (and how much they rely on GDAL). For instance, this NASA product, strongly centered in the modeler community is sometimes sloppy about these metadata conventions, and as a result GDAL (especially older versions), might not detect all the details appropriately. Note that GDAL has failed to recognize the units of lat-long, so we have had to subset the x-y positions manually."
  },
  {
    "objectID": "contents/python-ex.html",
    "href": "contents/python-ex.html",
    "title": "",
    "section": "",
    "text": "# Examining Environmental Justice through Open Source, Cloud-Native Tools: R\nThis Jupyter (ipynb) notebook provides a brief introduction to a cloud-native workflow. A more complete version of this example can be found in the intro.qmd Quarto notebook. Here, we merely include an ipynb version to demonstrate how Jupyter notebooks may be included in quarto.\nNote: in Codespaces editor, choose “Select Kernel” -&gt; “Jupyter Kernels” to get started.\nDeveloper note: At this time, Gitpod VSCode editor will not detect the default python environment (VIRTUAL_ENV variable, pointing to /opt/venv/bin/python), or any other kernelspec added with ipykernel.\n\nfrom pystac_client import Client\nimport odc.stac\nimport pystac_client\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nbox = [-122.51006, 37.70801, -122.36268, 37.80668]\nclient = Client.open(\"https://earth-search.aws.element84.com/v1\")\nsearch = client.search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}\n)\n\n\nitems = search.get_all_items()\n# items[0]  # peak at an item\n\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:32610\",\n    bands=[\"nir08\", \"red\"],\n    resolution=30,\n    bbox=box\n)\n\n\nred = data.red\nnir = data.nir08\n\n# summarize over time. \n# quite probably better to use resampling strategy in odc.stac.load though.\nimport dask.diagnostics\nwith dask.diagnostics.ProgressBar():\n    ndvi = ( ((nir - red) / (red + nir)).\n            resample(time=\"MS\").\n            median(\"time\", keep_attrs=True).\n            compute()\n    )\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=4)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fec26fd96c0&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NASA TOPS-T: Cloud Native Geospatial in R & Python",
    "section": "",
    "text": "This project seeks to introduce cloud-native approaches to geospatial analysis in R & Python through the lens of environmental justice applications. This is not meant as a complete course in geospatial analysis – though we encourage interested readers to consider Geocomputation in R or Python as an excellent resource. We present opinionated recipes meant to empower users with the following design goals:\nThis is a work in progress! Check back often, and feedback welcome! Test these modules, file an issue or pull request, launch into a Codespaces environment, or reach out on the discussion board.\nWhat is “cloud-native” anyway? We define cloud-native to mean simply that data is accessed over http range request methods, rather than downloading entire files. Code-based examples will develop why this is important and how it differs from renting cloud-based compute. The core philosophy is that what many users already know how to do locally translates pretty seamlessly here, and then a bit extra is required to coerce certain software to stay in ‘range request’ mode and not get greedy trying to download everything. Some authors define this concept somewhat differently, and certainly not all range requests give the same performance, nor are http range requests best in all cases."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "NASA TOPS-T: Cloud Native Geospatial in R & Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to input, suggestions, feedback and ideas from so many amazing folks in the OpenScapes community and input and financial support from the NASA TOPS community."
  },
  {
    "objectID": "contents/sst-gdal.html",
    "href": "contents/sst-gdal.html",
    "title": "",
    "section": "",
    "text": "#remotes::install_github(\"boettiger-lab/earthdatalogin\")\nlibrary(earthdatalogin)\n\n\nedl_netrc()\nurls &lt;- edl_search(short_name = \"MUR-JPL-L4-GLOB-v4.1\",\n                   temporal = c(\"2020-01-01\", \"2021-12-31\"))\n\n\nvrt &lt;- function(url) {\n  prefix &lt;-  \"vrt://NETCDF:/vsicurl/\"\n  suffix &lt;- \":analysed_sst?a_srs=OGC:CRS84&a_ullr=-180,90,180,-90\"\n  paste0(prefix, url, suffix)\n}\n\n\nlibrary(gdalcubes)\ngdalcubes_options(parallel = parallel::detectCores()*2)\n\n\nurl_dates &lt;- as.Date(gsub(\".*(\\\\d{8})\\\\d{6}.*\", \"\\\\1\", urls), format=\"%Y%m%d\")\ndata_gd &lt;- gdalcubes::stack_cube(vrt(urls), datetime_values = url_dates)\n\n\nextent = list(left=-93, right=-76, bottom=41, top=49,\n              t0=\"2020-01-01\", t1=\"2021-12-31\")\n\nbench::bench_time({\n  data_gd |&gt; \n    gdalcubes::crop(extent) |&gt; \n    aggregate_time(dt=\"P1M\", method=\"sd\") |&gt; \n    plot(col = viridisLite::viridis(10))\n})\n\n\n\n\nprocess    real \n  24.2s   14.5m"
  },
  {
    "objectID": "contents/intro.html",
    "href": "contents/intro.html",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools",
    "section": "",
    "text": "This executable notebook provides an opening example to illustrate a cloud-native workflow in both R and python. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow. Note the close parallels between R and Python syntax. This arises because both languages wrap the same underlying tools (the STAC API and GDAL warper) and handle many of the nuisances of spatial data – from re-projections and resampling to mosaic tiles – without us noticing."
  },
  {
    "objectID": "contents/intro.html#data-discovery",
    "href": "contents/intro.html#data-discovery",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools",
    "section": "Data discovery",
    "text": "Data discovery\n\nThe first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.\n\nRPython\n\n\n\nbox &lt;- c(xmin=-122.51, ymin=37.71, xmax=-122.36, ymax=37.81) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  ext_query(\"eo:cloud_cover\" &lt; 20) |&gt;\n  post_request()\n\n\n\n\nbox = [-122.51, 37.71, -122.36, 37.81]\nitems = (\n  Client.\n  open(\"https://earth-search.aws.element84.com/v1\").\n  search(\n    collections = ['sentinel-2-l2a'],\n    bbox = box,\n    datetime = \"2022-06-01/2022-08-01\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}).\n  item_collection()\n)\n\n\n\n\nWe pass this list of images to a high-level utilty (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)\n\nRPython\n\n\n\ncol &lt;- stac_image_collection(items$features, asset_names = c(\"B08\", \"B04\", \"SCL\"))\n\ncube &lt;- cube_view(srs =\"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  dx = 0.0001, dy = 0.0001, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\ndata &lt;-  raster_cube(col, cube, mask = mask)\n\n\n\n\ndata = odc.stac.load(\n    items,\n    crs=\"EPSG:4326\",\n    bands=[\"nir08\", \"red\"],\n    resolution=0.0001,\n    bbox=box\n)\n\n\n\n\nWe can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine tree cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming)\n\nRPython\n\n\n\nndvi &lt;- data |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\n\nndvi_stars &lt;- st_as_stars(ndvi)\n\n\n\n\nred = data.red\nnir = data.nir08\n\nndvi = (((nir - red) / (red + nir)).\n        resample(time=\"MS\").\n        median(\"time\", keep_attrs=True).\n        compute()\n)\n\n# mask out bad pixels\nndvi = ndvi.where(ndvi &lt;= 1)\n\n\n\n\nAnd we plot the result. The long rectangle of Golden Gate Park is clearly visible in the North-West.\n\nRPython\n\n\n\nmako &lt;- tm_scale_continuous(values = viridisLite::mako(30))\nfill &lt;- tm_scale_continuous(values = \"Greens\")\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako)\n\n\n\n\n\n\n\nimport matplotlib as plt\ncmap = plt.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nndvi.plot.imshow(row=\"time\", cmap=cmap, add_colorbar=False, size=5)\n\n&lt;xarray.plot.facetgrid.FacetGrid object at 0x7fd62008e9e0&gt;"
  },
  {
    "objectID": "contents/intro.html#zonal-statistics",
    "href": "contents/intro.html#zonal-statistics",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools",
    "section": "Zonal statistics",
    "text": "Zonal statistics\nIn addition to large scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis, and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarise the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI)\n\nRPython\n\n\n\nsf &lt;- st_read(\"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\") |&gt;\n  st_make_valid() |&gt; select(-label_coords)\npoly &lt;- ndvi |&gt; extract_geom(sf, FUN = mean, reduce_time = TRUE)\nsf$NDVI &lt;- poly$NDVI\n\n\n\n\nndvi.rio.to_raster(raster_path=\"ndvi.tif\", driver=\"COG\")\nsf_url = \"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/citiesData/CASanFrancisco1937/geojson.json\"\nmean_ndvi = zonal_stats(sf_url, \"ndvi.tif\", stats=\"mean\")\n\n/opt/venv/lib/python3.10/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\n\n\n\n\n\nWe plot the underlying NDVI as well as the average NDVI of each polygon, along with it’s textual grade, using tmap. Note that “A” grades tend to be darkest green (high NDVI) while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)\n\nRPython\n\n\n\ntm_shape(ndvi_stars) + tm_raster(col.scale = mako) +\n  tm_shape(sf) + tm_polygons('NDVI', fill.scale = fill) +\n  tm_shape(sf) + tm_text(\"grade\", col=\"darkblue\", size=0.6) +\n  tm_legend_hide()\n\n\n\n\n\n\n\nsf = gpd.read_file(sf_url)\nsf[\"ndvi\"] = [x[\"mean\"] for x in mean_ndvi ]\nsf.plot(column=\"ndvi\", legend=True)\n\n\n\n\n\n\n\nAre historically redlined areas still less green?\n\nRPython\n\n\n\nsf |&gt; \n  as_tibble() |&gt;\n  group_by(grade) |&gt; \n  summarise(ndvi = mean(NDVI), \n            sd = sd(NDVI)) |&gt;\n  knitr::kable()\n\n\n\n\ngrade\nndvi\nsd\n\n\n\n\nA\n0.3201204\n0.0611414\n\n\nB\n0.2138481\n0.0783211\n\n\nC\n0.1956285\n0.0564827\n\n\nD\n0.1949750\n0.0385796\n\n\nNA\n0.0962092\nNA\n\n\n\n\n\n\n\n\nimport geopolars as gpl\nimport polars as pl\n\n(gpl.\n  from_geopandas(sf).\n  group_by(\"grade\").\n  agg(pl.col(\"ndvi\").mean()).\n  sort(\"grade\")\n)\n\n\nshape: (5, 2)\n\n\n\ngrade\nndvi\n\n\nstr\nf64\n\n\n\n\nnull\n0.157821\n\n\n\"A\"\n0.338723\n\n\n\"B\"\n0.247344\n\n\n\"C\"\n0.231182\n\n\n\"D\"\n0.225696"
  },
  {
    "objectID": "contents/computing-environment.html",
    "href": "contents/computing-environment.html",
    "title": "Computing Environments",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud."
  },
  {
    "objectID": "contents/computing-environment.html#portable-reproducibility",
    "href": "contents/computing-environment.html#portable-reproducibility",
    "title": "Computing Environments",
    "section": "",
    "text": "Good reproducibility is like an onion – it comes in many layers. There’s a purpose to containerized environments deployed on cloud-hosted virtual machines. And we believe students should be able to leverage those things, easily and rapidly deploying cloud-hosted images, and will get to that here. But most of the time, we just want to copy-paste a few lines of code and expect it to work. Many layers of the onion can be found between these two extremes – from package dependencies and system dependencies to containers, orchestration, metadata, even hardware requirements.\nIn the examples here, copy-pasting the code blocks into your preferred environment should work in most cases. Sometimes it may be necessary to install specific libraries or specific versions of those libraries. And for fastest setup and maximum reproducibility, users can deploy the fully containerized environment. It should be as easy as possible to grab the whole onion and take it where you want it – be that a local VSCode editor on your laptop, or an RStudio Server instance running up on Microsoft Azure cloud."
  },
  {
    "objectID": "contents/computing-environment.html#notebooks-on-github",
    "href": "contents/computing-environment.html#notebooks-on-github",
    "title": "Computing Environments",
    "section": "Notebooks on GitHub",
    "text": "Notebooks on GitHub\nEach of the recipes on this site correspond to a Quarto or Jupyter notebook in a GitHub repository (see contents/ directory or footer links). Such notebooks form the basis of technical documentation and publishing to a wide array of formats. A _quarto.yml configuration file in the repository root determines the website layout. Notebooks can be run interactively in any appropriate environment (RStudio, JupyterLab, VSCode, etc, see below for free online platforms). Type quarto render from the bash command line or use quarto::quarto_preview() from the R console to preview the entire site."
  },
  {
    "objectID": "contents/computing-environment.html#a-portable-compute-environment",
    "href": "contents/computing-environment.html#a-portable-compute-environment",
    "title": "Computing Environments",
    "section": "A Portable compute environment",
    "text": "A Portable compute environment"
  },
  {
    "objectID": "contents/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "href": "contents/computing-environment.html#on-the-cloud-gitpod-or-codespaces",
    "title": "Computing Environments",
    "section": "On the cloud: Gitpod or Codespaces",
    "text": "On the cloud: Gitpod or Codespaces\n \nBoth GitHub Codespaces and Gitpod provide a fast and simple way to enter into integrated development environments such as VSCode, RStudio, or JupyterLab on free, cloud-based virtual machines. Codespaces has a free tier of 60 hours/month, Gitpod of 50 hours a month, both offer paid plans for additional use and larger compute instances. Small codespace instances are also free to instructors\nBy clicking on one of the buttons in GitHub, users will be placed into a free cloud-based virtual machine running a VSCode editor in their web browser. The first setup can take a few minutes to complete.\nAdditionally, this also provides access to an RStudio environment on an embedded port for users who prefer that editor to VSCode. Once the Codespace has fully completed loading, it will include a link in a second welcome message in the Terminal to access RStudio like so:\n\nThe RStudio link can also always be accessed from the Ports tab under the port labeled “Rstudio” (8787). (Gitpod will show a pop-up message to open this port instead.)\nBoth Codespaces and Gitpod can be configured with custom compute environments by supplying a docker image. Both the VSCode and RStudio editors run in the same underlying custom Docker container. This repository includes a Dockerfile defining this compute environment which includes specific versions of R and python packages, the latest releases of the OSGeo C libraries GDAL, PROJ, and GEOS that power many spatial operations in both languages. These elements are pre-compiled in a Docker container based on the latest Ubuntu LTS release (22.04 at the time of writing), which itself is build according to the Dockerfile found in this repository using a GitHub Action. The devcontainer.json configuration will also set up relevant VSCode extensions for both both Jupyter and Quarto notebooks, with each supporting both R and Python."
  },
  {
    "objectID": "contents/computing-environment.html#locally-vscode",
    "href": "contents/computing-environment.html#locally-vscode",
    "title": "Computing Environments",
    "section": "Locally: VSCode",
    "text": "Locally: VSCode\nOpen this repository in a local Visual Studio Code editor on a Mac, Linux, or Windows laptop and you will probably be prompted “Do you want to open this in project in a Dev Container?” If you agree, VSCode will attempt to use a local Docker installation to pull a container with much of the required software already installed. This uses the same Docker container and enables all the same extensions in VSCode, including RStudio server on the embedded port.\nOf course, users can open this project in a local VSCode or any other favorite editor without opening in the devcontainer. The user assumes responsibility to install necessary software, i.e. the packages listed in requirements.txt or install.R. Note that doing so does not ensure that the same version of system libraries like GDAL, PROJ, or GEOS will necessarily be used. For most operations this should not matter, but users on older versions of GDAL may encounter worse performance or other difficulties."
  },
  {
    "objectID": "contents/computing-environment.html#anywhere-docker",
    "href": "contents/computing-environment.html#anywhere-docker",
    "title": "Computing Environments",
    "section": "Anywhere: Docker",
    "text": "Anywhere: Docker\nWe can sidesteps elements specific to the VSCode editor defined in the devcontainer.json configuration while still leveraging the same system libraries and pre-built packages. For example, a user could also choose to run (or extend) the underlying docker container independently, e.g.\ndocker run --rm -ti ghcr.io/boettiger-lab/nasa-tops:latest bash\nwhich opens a bash terminal inside the container. This approach is also compatible with most HPC setups using singularity instead of docker.\nSome users may not be familiar with editing and running code entirely from a bash shell, so the container also includes RStudio server and thus can be run to launch RStudio in an open port instead,\ndocker run -d -p 8787:8787 --user root -e DISABLE_AUTH=true \\\n  ghcr.io/boettiger-lab/nasa-tops:latest\nand visit http://localhost:8787 to connect."
  },
  {
    "objectID": "contents/ace.html",
    "href": "contents/ace.html",
    "title": "ACE Data",
    "section": "",
    "text": "California Department of Fish and Wildlife (CDFW) provides a series of data products to identify “areas of conservation emphasis” (ACE), which are currently being used as part of California’s 30x30 conservation initiative. At a recent event at the California Academy, one of the CDFW scientists mentioned it would be nice to see how the Biodiversity Intactness Index (BII) values compare to the CDFW prioritization layers. The BII (sometimes called the local or LBII) is one of a handful of indicators of global biodiversity change highlighted by GEO BON. Various attempts to estimate this indicator have been made over the past decades – the most recent and most high-resolution one currently available (and updated annually) is the Impact Observatory product based on the approach of Newbold et al and the PREDICTS database, which is available in cloud-optimized format from the Planetary Computer STAC catalog.\nAside from drowning in acronyms, this task provides a great chance to apply the same tools seen in our intro at a larger spatial scale. We will average the rasterized 100m projections from the BII over each of the 63,890 polygon tiles used in the CDFW data. Even though we are now dealing with a raster layer that involves a very derived product (BII) from a different provider (Planetary Computer), and a much larger set of polygons from a different source and in different scale and projection, the process is almost identical to the intro example.\nlibrary(stars)\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(tmap)\nlibrary(dplyr)\n\ngdalcubes::gdalcubes_options(parallel=24*2)\nCDFW uses a hex-based tiling scheme for their data products. Note that these pre-date the modern H3 hex tiling system, so do not provide the magic of hierarchical (zoom level) tiles. CDFW makes the ACE GIS Data freely available, but in one large zip archive. The ACE data includes may different layers all using the same hex-based tiles. The layer we pull here draws from their summary rankings on Terrestrial Irreplacability of species biodiversity.\nHere I’ll use a public mirror on a Berkeley-based server for a more cloud-native access pattern so we don’t have to download the whole thing. We also index this in a simple stac catalog\nurl &lt;- \"/vsicurl/https://minio.carlboettiger.info/public-biodiversity/ACE_summary/ds2715.gdb\"\nace &lt;- st_read(url)\ntm_shape(ace) + tm_polygons(fill = \"RwiRankEco\", col=NA)\nFind Biodiversity Intactness Index COG tiles from Planetary Computer using STAC search:\nbox &lt;- st_bbox(st_transform(ace, 4326))\nitems &lt;-\n  stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt;\n  stac_search(collections = \"io-biodiversity\",\n              bbox = c(box),\n              limit = 1000) |&gt;\n  post_request() |&gt;\n  items_sign(sign_fn = sign_planetary_computer())\nCreated a desired cube view\ncube &lt;- gdalcubes::stac_image_collection(items$features, asset_names = \"data\")\nbox &lt;- st_bbox(ace)\nv &lt;- cube_view(srs = \"EPSG:3310\",\n               extent = list(t0 = \"2020-07-01\", t1 = \"2020-07-31\",\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n               dx = 100, dy=100, dt = \"P1M\")\n\nQ &lt;- raster_cube(cube,v)\nQuick plot of the data at requested cube resolution.\nQ |&gt; plot(col=viridisLite::viridis(30), nbreaks=31)\nExtract mean value of the BII for each hex.\nbii &lt;- Q |&gt; gdalcubes::extract_geom(ace, FUN=mean)"
  },
  {
    "objectID": "contents/ace.html#plots",
    "href": "contents/ace.html#plots",
    "title": "ACE Data",
    "section": "Plots",
    "text": "Plots\nView the average BII index values by ACE hexagon:\n\ntmap_mode(\"plot\")\n\n# color palette\nviridis &lt;- tm_scale_continuous(values = viridisLite::viridis(30))\n\nbii &lt;- ace |&gt;\n  tibble::rowid_to_column(\"FID\") |&gt; \n  left_join(bii)\n\ntm_shape(bii) + \n  tm_polygons(fill = \"data\", col=NA, fill.scale = viridis)\n\n\n\n\nLet’s zoom in manually:\n\ntmap_mode(\"plot\")\nsf = st_bbox(c(xmin=-123, ymin=38.5, xmax=-122, ymax=37.5), crs=4326)\ntm_shape(bii, bbox = sf) + \n  tm_polygons(fill = \"data\", col=NA,\n              fill.scale = viridis)\n\n\n\n\nNow that we have the data in convenient and familiar data structures, it is easy to analyze. On average, a hexagon’s irreplacability rank shows little correlation with BII:\n\nlibrary(tidyverse)\nbii |&gt; as_tibble() |&gt; select(-Shape) |&gt;\n  group_by(RwiRankEco) |&gt; \n  summarise(BII = mean(data, na.rm=TRUE),\n            sd = sd(data, na.rm=TRUE)) |&gt;\n  ggplot(aes(RwiRankEco, BII)) + \n  geom_col(aes(fill=RwiRankEco)) + \n  geom_linerange(aes(ymin = BII-2*sd, ymax = BII+2*sd), col = \"grey50\")"
  },
  {
    "objectID": "contents/ace.html#leaflet-map",
    "href": "contents/ace.html#leaflet-map",
    "title": "ACE Data",
    "section": "Leaflet map",
    "text": "Leaflet map\nWe can also render an interactive leaflet plot where we can zoom in and out and toggle basemaps:\ntmap_mode(\"view\")\nbii &lt;- Q |&gt; st_as_stars()\nmap &lt;- \n  tm_shape(bii) +\n  tm_raster(col.scale = viridis) +\n  tm_shape(ace) +\n  tm_polygons(fill = \"RwiRankEco\", col=NA)\n\ntmap_save(map, \"bii_hexes.html\")\ninteractive map"
  },
  {
    "objectID": "contents/earthdata.html",
    "href": "contents/earthdata.html",
    "title": "NASA EarthData",
    "section": "",
    "text": "The NASA EarthData program provides access to an extensive collection of spatial data products from each of its 12 Distributed Active Archive Centers (‘DAACs’) on the high-performance S3 storage system of Amazon Web Services (AWS). We can take advantage of range requests with NASA EarthData URLs, but unlike the previous examples, NASA requires an authentication step. NASA offers several different mechanisms, including netrc authentication, token-based authentication, and S3 credentials, but only the first of these works equally well from locations both inside and outside of AWS-based compute, so there really is very little reason to learn the other two.\nThe earthdatalogin package in R or the earthaccess package in Python handle the authentication. The R package sets up authentication behind the scenes using environmental variables.\n\nearthdatalogin::edl_netrc()\n\n(A default login is supplied though users are encouraged to register for their own individual accounts.) Once this is in place, EarthData’s protected URLs can be used like any other:\n\nterra::rast(\"https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T56JKT.2023246T235950.v2.0/HLS.L30.T56JKT.2023246T235950.v2.0.SAA.tif\",\n     vsi=TRUE)\n\nclass       : SpatRaster \ndimensions  : 3660, 3660, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 199980, 309780, 7190200, 7300000  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 56N (EPSG:32656) \nsource      : HLS.L30.T56JKT.2023246T235950.v2.0.SAA.tif \nname        : HLS.L30.T56JKT.2023246T235950.v2.0.SAA"
  },
  {
    "objectID": "contents/multifile_raster.html",
    "href": "contents/multifile_raster.html",
    "title": "Multifile raster arrays",
    "section": "",
    "text": "pythonR\n\n\n\nimport requests\nfrom lxml.html import parse\nfrom io import StringIO\n\nimport xarray as xr\nfrom datetime import datetime\nimport rioxarray # used magically via xr\nimport dask\n\n\n\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(stars)\n\n\n\n\nWe extract some list of urls to netcdf files:\n\npythonR\n\n\n\nurl = \"https://noaadata.apps.nsidc.org/NOAA/G02202_V4/north/daily/1978/\"\nr = requests.get(url) \ntree = parse(StringIO(r.text)).getroot()\nfiles = [a.text for a in tree.iter('a')]\nsic_files = [i for i in files if 'conc' in i]\nsic_urls = [url + s for s in sic_files]\n\n\n\n\nurl = \"https://noaadata.apps.nsidc.org/NOAA/G02202_V4/north/daily/1978/\"\npage &lt;- read_html(url)\nfiles &lt;- page %&gt;% html_nodes(\"a\") %&gt;% html_attr(\"href\")\nurls &lt;- paste0(url, files)\nsic_urls &lt;- urls |&gt; subset(str_detect(urls, \"conc\"))\n\n\n\n\nWe open all these netcdf files lazily over the virtual filesystem, allowing us to efficiently subset just what we need.\n\nPythonR\n\n\n\ncdr = xr.open_mfdataset(sic_urls, \n                        variable=\"cdr_seaice_conc\", \n                        engine=\"rasterio\", \n                        concat_dim=\"time\", \n                        combine=\"nested\")\n\n\n\n\ndates &lt;- stringr::str_extract(sic_urls, \"\\\\d{8}\") |&gt; lubridate::ymd()\n\ncdr &lt;- read_stars(paste0(\"/vsicurl/\", sic_urls),\n                  \"cdr_seaice_conc\", \n                  along = list(time = dates), \n                  quiet = TRUE)"
  },
  {
    "objectID": "contents/sst.html",
    "href": "contents/sst.html",
    "title": "EarthData strategies",
    "section": "",
    "text": "knitr::opts_chunk$set(message=FALSE, warning=FALSE)\nIn this example, we have over 13K distinct points where turtles have been sampled over many years, and we wish to extract the sea surface temperature at each coordinate point. This task is somewhat different than ones we have considered previously, because instead of extracting data from a continuous cube in space & time, we have only discrete points in space and time we wish to access. Downloading files representing entirety of space or continuous ranges in time is thus particularly inefficient. Here we will try and pluck out only the sample points we need.\nThis design is a nice chance to illustrate workflows that depend directly on URLs, without leveraging the additional metadata that comes from STAC.\nlibrary(earthdatalogin)\nlibrary(rstac)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(tmap)\nlibrary(gdalcubes)\n# handle earthdata login\nedl_netrc()\nWe begin by reading in our turtle data from a spreadsheet and converting the latitude/longitude columns to a spatial vector (spatial points features) object using sf:\nturtles &lt;- \n  read_csv(\"https://raw.githubusercontent.com/nmfs-opensci/NOAAHackDays-2024/main/r-tutorials/data/occ_all.csv\",\n           show_col_types = FALSE) |&gt; \n  st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"))\n\nst_crs(turtles) &lt;- 4326 # lon-lat coordinate system\n\n\ndates &lt;- turtles |&gt; distinct(date) |&gt; pull(date)\nWe have 13222 data points, including 1695 unique dates that stretch from 2004-12-14 to 2019-10-17.\nLet’s take a quick look at the distribution of the data (coloring by date range):\n# Quick plot of the turtle data\npal &lt;- tmap::tm_scale_ordinal(5)\ntm_basemap(\"CartoDB.DarkMatter\") + \n  tm_shape(turtles) + tm_dots(\"date\", fill.scale = pal)\nSearching for NASA sea-surface-temperature data is somewhat tricky, since the search interfaces take only date ranges, not specific dates. The SST data is organized as one netcdf file with global extent for each day, so we’ll have to request all URLs in a date range and then filter those down to the URLs matching the dates we need.\nNASA’s STAC search is unfortunately much slower and more prone to server errors than most STAC engines at present. NASA provides its own search API, which is substantially faster and more reliable, but does not provide metadata in a widely recognized standard. Here we will get 1000s of URLs, covering every day in this nearly 15 year span.\nurls &lt;- edl_search(short_name = \"MUR-JPL-L4-GLOB-v4.1\",\n                   temporal = c(min(turtles$date), max(turtles$date)))\nNow we subset URLs to only those dates that are found in turtles data.\nurl_dates &lt;- as.Date(gsub(\".*(\\\\d{8})\\\\d{6}.*\", \"\\\\1\", urls), format=\"%Y%m%d\")\nurls &lt;- urls[ url_dates %in% dates ]\nOkay, we have 1695 URLs now from which to extract temperatures at our observed turtle coordinates."
  },
  {
    "objectID": "contents/sst.html#a-partial-approach-via-stars",
    "href": "contents/sst.html#a-partial-approach-via-stars",
    "title": "EarthData strategies",
    "section": "A partial approach, via stars:",
    "text": "A partial approach, via stars:\nThis approach works on a subset of URLs, unfortunately stars is not particularly robust at reading in large numbers of URLs\n\nsome_urls &lt;- urls[1:50]\nsome_dates &lt;- as.Date(gsub(\".*(\\\\d{8})\\\\d{6}.*\", \"\\\\1\", some_urls), format=\"%Y%m%d\")\n# If we test with a subset of urls, we need to test with a subset of turtles too!\ntiny_turtle &lt;- turtles |&gt; filter(date %in% some_dates)\n\nbench::bench_time({ \n  sst &lt;- read_stars(paste0(\"/vsicurl/\", some_urls), \"analysed_sst\", \n                    #along = list(time = some_dates),  ## \n                    quiet=TRUE)\n  st_crs(sst) &lt;- 4326  # Christ, someone omitted CRS from metadata\n  # before we can extract on dates, we need to populate this date information\n  sst &lt;- st_set_dimensions(sst, \"time\", values = some_dates)\n})\n\nprocess    real \n  5.58s   1.16m \n\nbench::bench_time({\n  turtle_temp &lt;- st_extract(sst, tiny_turtle, time_column = \"date\")\n})\n\nprocess    real \n 15.73s   4.58m"
  },
  {
    "objectID": "contents/sst.html#gdalcubes-a-more-scalable-solution",
    "href": "contents/sst.html#gdalcubes-a-more-scalable-solution",
    "title": "EarthData strategies",
    "section": "gdalcubes – A more scalable solution",
    "text": "gdalcubes – A more scalable solution\n\nlibrary(gdalcubes)\ngdalcubes_set_gdal_config(\"GDAL_NUM_THREADS\", \"ALL_CPUS\")\ngdalcubes_options(parallel = TRUE)\n\nAccess to NASA’s EarthData collection requires an authentication. The earthdatalogin package exists only to handle this!\nUnlike sf, terra etc, the way gdalcubes calls gdal does not inherit global environmental variables, so this helper function sets the configuration.\n\nearthdatalogin::with_gdalcubes()\n\nUnfortunately, NASA’s netcdf files lack some typical metadata regarding projection and extent (bounding box) of the data. Some tools are happy to ignore this, just assuming a regular grid, but because GDAL supports explicitly spatial extraction, it wants to know this information. Nor is this information even provided in the STAC entries! Oh well – here we provide it rather manually using GDAL’s “virtual dataset” prefix-suffix syntax (e.g. note the a_srs=OGC:CRS84), so that GDAL does not complain that the CRS (coordinate reference system) is missing. Additional metadata such as the timestamp for each image is always included in a STAC entry and so can be automatically extracted by stac_image_collection. (stars is more forgiving about letting us tack this information on later.)\n\nvrt &lt;- function(url) {\n  prefix &lt;-  \"vrt://NETCDF:/vsicurl/\"\n  suffix &lt;- \":analysed_sst?a_srs=OGC:CRS84&a_ullr=-180,90,180,-90\"\n  paste0(prefix, url, suffix)\n}\n\nNow we’re good to go. We create the VRT versions of the URLs to define the cube. We can then extract sst data at the point coordinates given by turtle object.\n\ndatetime &lt;- as.Date(gsub(\".*(\\\\d{8})\\\\d{6}.*\", \"\\\\1\", urls), format=\"%Y%m%d\")\ncube &lt;- gdalcubes::stack_cube(vrt(urls), datetime_values = datetime)\n\nbench::bench_time({\n  \nsst_df &lt;- cube |&gt; extract_geom(turtles,  time_column = \"date\")\n\n})\n\nprocess    real \n  8.65s   4.51m \n\n\nThe resulting data.frame has the NASA value for SST matching the time and space noted noted in the data. The NetCDF appears to encodes temperatures to two decimal points of accuracy by using integers with a scale factor of 100 (integers are more compact to store than floating points), so we have to convert these. There are also what looks like some spurious negative values that may signal missing data.\n\n# re-attach the spatial information\nturtle_sst &lt;- \n  turtles |&gt; \n  tibble::rowid_to_column(\"FID\") |&gt;\n  inner_join(sst_df, by=\"FID\") |&gt; \n  # NA fill and convert to celsius\n  mutate(x1 = replace_na(x1, -32768),\n         x1 = case_when(x1 &lt; -300 ~ NA, .default = x1),\n         nasa_sst = (x1 + 27315) * 0.001)\n\n\npal &lt;- tmap::tm_scale_continuous(5, values=\"hcl.blue_red\")\ntm_basemap(\"CartoDB.DarkMatter\") + \n  tm_shape(turtle_sst) + tm_dots(\"nasa_sst\", fill.scale = pal)"
  },
  {
    "objectID": "contents/R-ex.html",
    "href": "contents/R-ex.html",
    "title": "Examining Environmental Justice through Open Source, Cloud-Native Tools: R",
    "section": "",
    "text": "This Jupyter (ipynb) notebook provides a brief introduction to a cloud-native workflow. A more complete version of this example can be found in the intro.qmd Quarto notebook. Here, we merely include an ipynb version to demonstrate how Jupyter notebooks may be included in quarto and, in this case, execute in an R kernel.\nNote: in Codespaces editor, choose “Select Kernel” -&gt; “Jupyter Kernels” -&gt; “R” to get started.\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\ngdalcubes::gdalcubes_options(parallel = TRUE)\n\n\nbox &lt;- c(xmin=-122.51006, ymin=37.70801, xmax=-122.36268, ymax=37.80668) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\nitems &lt;-\n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  post_request()\n\n\n\ncol &lt;- stac_image_collection(items$features,\n                             asset_names = c(\"B04\", \"B08\", \"SCL\"),\n                             property_filter = \\(x){\n                               x[[\"eo:cloud_cover\"]] &lt; 20\n                             })\n\ncube &lt;- cube_view(srs = \"EPSG:4326\",\n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  nx = 1200, ny = 1200, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nmask &lt;- image_mask(\"SCL\", values=c(3, 8, 9)) # mask clouds and cloud shadows\n\n\n\nndvi &lt;- raster_cube(col, cube, mask = mask) |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\"))\nplot(ndvi)"
  },
  {
    "objectID": "contents/sst-xarray.html",
    "href": "contents/sst-xarray.html",
    "title": "Using fsspec virtual filesystem",
    "section": "",
    "text": "import earthaccess\nimport rioxarray\nimport rasterio\nimport xarray as xr\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nresults = earthaccess.search_data(\n    short_name=\"MUR-JPL-L4-GLOB-v4.1\",\n    #temporal=(\"2020-01-01\", \"2021-12-31\"), # 2 years, ~ 500 GB\n    temporal=(\"2019-01-01\", \"2019-01-31\"),\n)\n\nGranules found: 31\nRather than tediously downloading the entirety of each file and then manually looping over each one to open, read, and concatinate as shown in the Coiled Demo, we can simply open the whole set in one go. This lazy read method allows us to then range-request only the subset of data we need from each file, thanks to earthaccess using a cloud-native reads over http via the python fsspec package. We can then issue the usual xarray operations to process and plot the data, treating the remote source as if it were already sitting on our local disk. This approach is quite fast, works from any machine, and does not require spending money on AWS. (note that fsspec package, which is doing the core magic of allowing us to treat the remote filesystem as if it were a local filesystem, is not explicitly visible in this workflow, but earthaccess has taken care of it for us).\n%%time\nfiles = earthaccess.open(results)\nds = xr.open_mfdataset(files,\n                       decode_times=False, \n                       data_vars=['analysed_sst', 'sea_ice_fraction'], \n                       concat_dim=\"time\", \n                       combine=\"nested\",\n                       parallel=True)\n\nOpening 31 granules, approx size: 11.63 GB\nCPU times: user 5.48 s, sys: 1.82 s, total: 7.3 s\nWall time: 1min 10s\n%%time\ndds = ds.sel(lon=slice(-93, -76), lat=slice(41, 49))\ncond = (dds.sea_ice_fraction &lt; 0.15) | np.isnan(dds.sea_ice_fraction)\nresult = dds.analysed_sst.where(cond)\n\nCPU times: user 19.6 ms, sys: 0 ns, total: 19.6 ms\nWall time: 17.9 ms\n%%time\nresult.std(\"time\").plot(figsize=(14, 6), x=\"lon\", y=\"lat\")\n\nCPU times: user 8.95 s, sys: 4.1 s, total: 13 s\nWall time: 1min 38s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f1eac34e440&gt;"
  },
  {
    "objectID": "contents/sst-xarray.html#using-gdal-virtual-filesystem",
    "href": "contents/sst-xarray.html#using-gdal-virtual-filesystem",
    "title": "Using fsspec virtual filesystem",
    "section": "Using GDAL Virtual Filesystem",
    "text": "Using GDAL Virtual Filesystem\nA different virtual filesystem approach is available through GDAL. While fsspec tries to provide a generic POSIX-like interface to remote files, the GDAL VSI is specifically optimized for spatial data and often considerably faster. The rioxarray package provides a drop-in engine to xarray’s open_mfdataset that uses GDAL. (Aside – at least some of the other netcdf engines supported by xarray should also be able to natively perform range requests over URLs to data without needing the fsspec layer added by earthaccess, and may have better performance. This case is not illustrated in this notebook). Here we’ll use the GDAL VSI.\nBecause the NASA EarthData are behind a security layer, using the URLs directly instead of earthaccess with fsspec requires a little extra handling of authentication process to make GDAL aware of the NETRC and cookie files it needs. We’ll also set some of the optional but recommended options for GDAL when using the virtual filesystem. Unfortunately this makes our code look a bit verbose – ideally packages like rioxarray would take care of these things.\n\n%%time\nimport os\nfrom pathlib import Path\ncookies = os.path.expanduser(\"~/.urs_cookies\")\nPath(cookies).touch()\n\n## pull out the URLs\ndata_links = [granule.data_links(access=\"external\") for granule in results]\nurl_links = [f'{link[0]}' for link in data_links]\n\n\n# and here we go\nwith rasterio.Env(GDAL_INGESTED_BYTES_AT_OPEN=\"32000\",\n                  GDAL_HTTP_MULTIPLEX=\"YES\",\n                  GDAL_HTTP_MERGE_CONSECUTIVE_RANGES=\"YES\",\n                  GDAL_HTTP_VERSION=\"2\",\n                  GDAL_NUM_THREADS=\"ALL_CPUS\",\n                  GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n                  GDAL_HTTP_COOKIEFILE=cookies, \n                  GDAL_HTTP_COOKIEJAR=cookies, \n                  GDAL_HTTP_NETRC=True):\n    ds1 = xr.open_mfdataset(url_links, \n                           engine = \"rasterio\", \n                           concat_dim=\"time\", \n                           combine=\"nested\",\n                           )\n\nCPU times: user 1.78 s, sys: 260 ms, total: 2.04 s\nWall time: 58.1 s\n\n\n\n%%time\ndds = ds1.sel(x=slice(18000-9300, 18000-7600), y = slice(9000+4100,9000+4900))\ndds.analysed_sst.std(\"time\").plot(figsize=(14, 6), x=\"x\", y=\"y\")\n\nCPU times: user 7.39 s, sys: 2.59 s, total: 9.98 s\nWall time: 42.3 s\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7f1e2c7254e0&gt;"
  },
  {
    "objectID": "contents/sst-xarray.html#comparisons",
    "href": "contents/sst-xarray.html#comparisons",
    "title": "Using fsspec virtual filesystem",
    "section": "Comparisons",
    "text": "Comparisons\nThe GDAL VSI is already widely used under the hood by python packages working with cloud-optimized geotiff (COG) files (e.g. via odc.stac, which like the above approach also produces dask-backed xarrays), and also widely used by most other languages (e.g. R) for working with any spatial data. To GDAL, netcdf and other so-called “n-dimensional array” formats like h5, zarr are just a handful of the 160-odd formats of “raster” data it supports, along with formats like COG and GeoTIFF files. It can be particularly powerful in more complicated workflows which require spatially-aware operations such as reprojection and aggregation. The GDAL VSI can sometimes be considerably faster than fsspec, expecially when configured for cloud-native access. The nusiance of these environmental variables aside, it can also be considerably easier to use and to generalize patterns across data formats (netcdf, zarr, COG), and across languages (R, C++, javascript, julia etc), since GDAL understands [all these formats] and is used in all of these languages, as well as in platforms such as Google Earth Engine and QGIS. This makes it a natural bridge between languages. This broad use over decades has made GDAL very powerful, and it continues to improve rapidly with frequent releases.\nFor some reason, the xarray community seems to prefer to access ncdf without GDAL, whether by relying on downloading complete files, using fsspec, or other dedicated libraries (zarr). There are possibly many reasons for this. One is a divide between the the “Geospatial Information Systems” community, that thinks of file serializations as “rasters” or “vectors”, and the “modeler” community, which thinks of data as “n-dimensional arrays”. Both have their weaknesses and the lines are frequently blurred, but one obvious manifestation is in how each one writes their netcdf files (and how much they rely on GDAL). For instance, this NASA product, strongly centered in the modeler community is sometimes sloppy about these metadata conventions, and as a result GDAL (especially older versions), might not detect all the details appropriately. Note that GDAL has failed to recognize the units of lat-long, so we have had to subset the x-y positions manually."
  }
]