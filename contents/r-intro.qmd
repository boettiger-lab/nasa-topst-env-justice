---
title: "Intro"
format: html
---

***WORK IN PROGRESS***

```{r}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, error=FALSE)
```

# Overview

This module provides an introduction to concepts in cloud-native geospatial analyses through the lens of environmental justice.  
This module assumes familiarity with the R computing environment, ideally through the core concepts of reading, visualizing, and manipulating data as introduced in [R For Data Science](https://r4ds.hadley.nz/). 
While prior experience in geospatial tooling is not assumed, this module is not intended to be a complete tutorial on geospatial concepts -- please consider the text [Geocomputation with R](https://r.geocompx.org/) for a more thorough treatment.



```{r setup}
library(tidyverse)
library(rstac)
library(gdalcubes)
library(stars)
library(tmap) # must be v4, install using remotes::install_github('r-tmap/tmap')
gdalcubes::gdalcubes_options(parallel = TRUE)
```


# Spatial Vector Objects

Spatial data objects come in essentially two flavors: **vector** and **raster** objects.  
Spatial **vector** data can be thought of as a special case of a tabular data (or relational database) structure, consisting of rows and columns.  
Each row represents a "feature" defined by explicit spatial coordinate type such as a point, line, or polygon, as indicated in a special "geometry" column.
Additionally, all spatial data objects (vectors or rasters) include core spatial metadata information, indicating the spatial projection 

We read in geospatial vector data which uses polygons to represent each housing area assessed by the Home Owners Loan Corporation

```{r}
CASanFrancisco1937 <- "https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/CASanFrancisco1937.geojson"

redlines <- 
  paste0("/vsicurl/", CASanFrancisco1937) |>
  st_read() |> 
  st_make_valid() # fix mangled/incomplete polygons
```

Note how we add the prefix `/vsicurl/` in front of the URL.  Under the hood, nearly all of our spatial libraries call the [GDAL](https://gdal.org/user/virtual_file_systems.html) C library to do the heavy lifting of reading and working with the 100s of different file formats used to represent geospatial vector and raster data.  Using this prefix tells GDAL to use the "[Virtual Filesystem](https://gdal.org/user/virtual_file_systems.html)" rather than download the entire data object.


We can get a quick visualization of these polygons overlaid on an interactive (Javascript leaflet) map by setting the `tmap` mode to `"view"`. 

```{r}

# Use a color scheme for grades based on HOLC maps
colors <-  tm_scale_categorical(values=c(A = "#729765",
                                         B = "#75a4b3",
                                         C = "#c6c167",
                                         D = "#b0707b" ))


tmap_mode("view")
tm_shape(redlines) + 
  tm_polygons("holc_grade", fill.scale = colors, fill_alpha = 0.6)
```

We can also make rich static plots with `tmap`, e.g. 

```{r}
tmap_mode("plot")


  tm_basemap(providers$CartoDB.Positron) + 
  tm_shape(redlines) + 
  tm_borders(col="holc_grade", 
             lwd = 4, 
             col.scale = colors) +
  tm_text(text = "holc_grade", size=0.5, col="holc_grade") +
  tm_legend_hide()

```


# Spatial Rasters


Spatial rasters represent data on discrete regular grid cells (pixels). 
Satellite imagery is a common source of spatial raster data that quickly becomes very very large, making the cloud native approaches such as the virtual filesystem mentioned above quite essential.

```{r}
## STAC Search over 400 million assets.

# manual bounding box
# box <-  c(xmin=-122.51006, ymin=37.70801, xmax=-122.36268, ymax=37.80668)
# or just extract bounding box from city map:
box <- st_bbox(redlines)  
start_date <- "2022-06-01"
end_date <- "2022-08-01"
items <- 
  stac("https://earth-search.aws.element84.com/v0/") |>
  stac_search(collections = "sentinel-s2-l2a-cogs",
              bbox = c(box),
              datetime = paste(start_date, end_date, sep="/"),
              limit = 100) |>
  post_request() 
```

## Low-level approach

Here we manually explore some of the information returned by the STAC catalog ...

```{r}
# 12 matching features
length(items$features)
```

```{r}
items$features[[1]] |> names()
```

```{r}
items$features[[1]]$assets |> names()
```

```{r}
b04 <- items$features[[1]]$assets$B04 
c(b04$title, b04$type)
```


Manually assemble the visual red-green-blue bands

```{r stars_vis}
i <- 11

blue <- paste0("/vsicurl/", items$features[[i]]$assets$B02$href)
green <- paste0("/vsicurl/", items$features[[i]]$assets$B03$href)
red <- paste0("/vsicurl/", items$features[[i]]$assets$B04$href)

vis <- read_stars(c(red,green,blue)) |> merge()
plot(vis, rgb=1:3)
```

Note that even seemingly simple tasks like cropping to a spatial subset require manual re-projection first, and this is relatively computationally intensive. This process will be much faster when we use the approach of `gdalcubes` instead.  

```{r st_transform}
# Note this low-level interace won't automatically deal with CRS transformations, but throws:
# Error in st_crop.stars_proxy(vis, redlines) :
# for cropping, the CRS of both objects has to be identical

# vis |> st_crop(redlines)

# We must manually reproject, which is slow and resource intensive
# We'll avoid this by using gdalcubes which allows the warper to handle this instead

# vis_sf <- vis |> st_transform(st_crs(redlines)) |> st_crop(redlines)
```


```{r ggplot}
# Don't try this, will definitely crash!
#ggplot() + geom_stars(data = vis) + scale_fill_identity()
```

We can still do network-based

```{r stars_ndvi}
red <- read_stars(red)
b08 <- paste0("/vsicurl/", items$features[[i]]$assets$B08$href)
nir <- read_stars(b08)
ndvi <- (nir - red) / (nir + red)
plot(ndvi)
```

This approach does not mask clouds and fill or average over other images -- we've used only one of the matching assets so far. While a single image here covers our area of interest (AOI),  in an expanded spatial analysis we would need to tile together overlapping images to ensure we cover the full AOI.  This also operates at native resolution, which can hurt performance when scaling to larger AOI.



## Newer cloud-native approach

Rather than manually parse the STAC catalog information, we can simply pass this metadata on to an intelligent function that knows how to use this.  We specify what bands of interest and request that any images where the metadata indicates over 20% cloud cover be dropped from the collection.  

```{r}
col <-
  stac_image_collection(items$features,
                        asset_names = c("B02", "B03", "B04","B08", "SCL"),
                        property_filter = \(x) {x[["eo:cloud_cover"]] < 20})
```


Seperately, we define an abstract "data cube" indicating how we want our data to look -- the bounding box in space and time, and the spatial/temporal resolution we want for the data.  

```{r}
cube <- cube_view(srs = "EPSG:4326",  
                  extent = list(t0 = start_date, t1 = end_date,
                                left = box[1], right = box[3],
                                top = box[4], bottom = box[2]),
                  nx = 2400, ny = 2400, dt = "P1M",
                  aggregation = "median", resampling = "average")
```

Third, we create an image mask indicating which pixels should be removed by using the data quality control layer, which indicates pixels that post-processing had identified as corresponding to clouds or cloud shadows:


```{r}
S2.mask <- image_mask("SCL", values=c(3,8,9)) # mask clouds and cloud shadows
```

```{r}
raster_cube(col, cube, mask = S2.mask) |>
  select_bands(c("B02", "B03", "B04")) |>
  aggregate_time(dt="P1M") |>
  plot(rgb=3:1)

```

```{r}

ndvi <- raster_cube(col, cube, mask = S2.mask) |>
  select_bands(c("B04", "B08")) |>
  apply_pixel("(B08-B04)/(B08+B04)", "NDVI") |>
  reduce_time(c("mean(NDVI)")) |>
  st_as_stars()

```


```{r}
tm_shape(ndvi) + 
  tm_raster(col.scale = tm_scale_continuous(values = viridisLite::mako(30)))

```





```{r}
tm_shape(ndvi) + 
  tm_raster(col.scale = tm_scale_continuous(values = viridisLite::mako(30))) +
  tm_shape(redlines) + tm_borders(col="holc_grade", lwd = 4, 
                                  col.scale = tm_scale_categorical(values=c(
                                    A = "#729765",
                                    B = "#75a4b3",
                                    C = "#c6c167",
                                    D = "#b0707b" ))) +
  tm_text(text = "holc_grade", size = 0.5, col = "holc_grade")

```




```{r}
```


```{r}
# more scalable performance with gdalcubes::extract_geom
ndvi_aves <- raster_cube(col, cube, mask = S2.mask) |>
  select_bands(c("B04", "B08")) |>
  apply_pixel("(B08-B04)/(B08+B04)", "NDVI") |>
  reduce_time(c("mean(NDVI)")) |>
  extract_geom(redlines, FUN=mean)

```

```{r}
ave_ndvi <- redlines |> 
  rowid_to_column("FID") |>
  left_join(ndvi_aves) 

ave_ndvi |>
  as_tibble() |>
  group_by(holc_grade) |>
  summarise(mean = mean(NDVI_mean))
```


```{r}
# "standard method"
# aves <- stars::st_extract(ndvi, redlines, FUN=mean)

# vec <- as_tibble(aves) |> left_join(redlines)
#vec |> group_by(holc_grade) |>
#   summarise(ndvi = mean(NDVI_mean, na.rm=TRUE))
```

```{r}
tmap_mode("view")
tm_shape(ave_ndvi) + 
  tm_polygons("NDVI_mean", 
              fill_alpha = 0.8,
              fill.scale = tm_scale_continuous(values = "Greens")) +
  tm_shape(redlines) + 
  tm_borders(col="holc_grade", 
             lwd = 4, 
             col.scale = tm_scale_categorical(values=c(
                A = "#729765",
                B = "#75a4b3",
                C = "#c6c167",
                D = "#b0707b" ))) +
  tm_text(text = "holc_grade", size = 0.5)
```